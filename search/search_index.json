{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>My name is Chris and I created this project as part of my directed study under Dr. Frank Wood in the Fall of 2025.</p>"},{"location":"#about-this-documentation","title":"About This Documentation","text":"<p>My goal of this technical report is to attempt to capture the 3 months I spent building this project such that if you were to start from nothing, you could probably do everything that I have done in the span of a week. I want to acknowledge that I cut some corners for the sake of speed.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>I am indebted to everyone on the PLAICraft project, who paved a lot of the way for this project to happen. I worked on PLAICraft for about 6 months, and I would say that the best way to describe my project is that it's a PLAICraft for anything.</p>"},{"location":"#what-to-expect","title":"What to Expect","text":"<p>I will try to keep these docs exciting by attempting to be funny. Sorry in advance. I am incredibly excited to dump the wealth of notes I have taken, and explain exactly how I overcame the various \"gotchas\" in building a scalable cloud framework for capturing training data for desktop agents.</p> <p>Let's get started.</p>"},{"location":"Introduction/amazon-web-services/","title":"Amazon Web Services","text":"<p>AWS was the cloud provider that I used, and from this point forward, I will not be using brand-agnostic terms. With that in mind, here are a few of the services and their jargon that you should know about:</p>"},{"location":"Introduction/amazon-web-services/#ec2","title":"EC2","text":"<p>These are Amazon's virtual machines. You pick the OS, the hardware, the storage, the security groups, the IAM roles. It's a computer in the cloud.</p>"},{"location":"Introduction/amazon-web-services/#ami","title":"AMI","text":"<p>If you take a snapshot of an EC2, it's like Amazon takes every byte exactly where it is at a given time and freezes it into an AMI. It is super easy to make one and then create new EC2's from it. Think of it as a saved file that you can clone infinitely.</p>"},{"location":"Introduction/amazon-web-services/#iam","title":"IAM","text":"<p>These are permissions in AWS. Wanna write to a database? You need IAM permissions for that. Wanna read a database? There's a SEPARATE IAM permission for that. If any services will interact with each other (and they will a whole damn lot), they need IAM permissions. Permissions are inside a Policy which is inside a Role.</p>"},{"location":"Introduction/amazon-web-services/#s3","title":"S3","text":"<p>Basically Google Drive. You can write to it via a library called <code>boto3</code>, which is basically AWS tools that lets you interact with AWS outside of AWS. The basic unit of S3 is a bucket.</p>"},{"location":"Introduction/amazon-web-services/#dynamodb","title":"DynamoDB","text":"<p>A database service. It's NoSQL, meaning that you don't interact with it via a query language like SQL. In my opinion, this just makes it easier to view / interact with than some of AWS's SQL solutions like RDS.</p>"},{"location":"Introduction/amazon-web-services/#lambda","title":"Lambda","text":"<p>Lambdas are sick. They're basically serverless and stateless chunks of code that you can trigger in various ways (e.g. EventBridge). If you want to upload a new hot dog recipe to your S3 and your DynamoDB, one lambda can do both. They're like spaghetti strands that make sure information flows through your application.</p>"},{"location":"Introduction/amazon-web-services/#api-gateway","title":"API Gateway","text":"<p>This is how I called Lambdas from the frontend. You set up an endpoint that your app can hit like <code>https://aws.gateway123/join/schedule</code> and when you hit it, you decide what it does. I had it invoke lambdas with whatever payload my frontend sent.</p> <p>Note: If I was hosting my domain on AWS, I would probably use something like Route 53, but I used Cloudflare. I'll get to that later.</p>"},{"location":"Introduction/jitsi-meet/","title":"Jitsi Meet","text":"<p>Jitsi itself is an open-source project containing several repositories. The only real important one for the sake of this project is Jitsi Meet. This is the main video conferencing application that has been extremely well-maintained and well-documented. It comes in a few forms via their docs, including downloading as a complete docker file that Just Works\u2122. However, you probably don't want that if, like me, you don't really understand Docker and its limitations.</p>"},{"location":"Introduction/jitsi-meet/#setup-advice","title":"Setup Advice","text":"<p>My advice: before even setting up a Jitsi server, have a purchased domain ready from a major provider like Cloudflare. Also, don't try to run peer-to-peer or locally. I managed to run my own Jitsi server from my own machine, but it requires access to your router so that you can enable port-forwarding ports. I probably spent two weekends trying to use a relay to access my server via STUN/TURN servers. Just skip all that, get AWS free tier, and install it on an EC2.</p> <p>Holy crap, the handbook actually has that exact advice. Too bad I didn't listen. But you can, and you should.</p>"},{"location":"Introduction/nice-dcv/","title":"NICE DCV","text":"<p>Amazon DCV (previously NICE DCV) is a remote desktop protocol (RDP). RDP lets you take remote control of someone else's computer, while having their screen streamed directly to you. The main app for doing this is literally just called Windows RDP. DCV costs nothing, it's like a package that unlocks extra functionality on EC2 instances.</p> <p>So why did Amazon acquire DCV for a bunch of money? Here are a few things that are special about DCV:</p> <ol> <li>Multi-user support: Several users can remote in at the same time, Windows RDP only allows one session</li> <li>Cross-platform: Supported on the three main operating systems (MacOS, Linux, Windows)</li> <li>High performance: DCV supports 4K and 60 FPS, and will compress the image quality in order to maintain low latency</li> <li>Dynamic resizing: DCV can resize the remote session based on the size of the end user's client</li> </ol> <p>All these things make DCV an interesting choice for companies that have expensive, highly customized workstations because they can simply remote into that machine. It also is good for gaming and is the remote desktop protocol that supports PLAICraft.</p>"},{"location":"Introduction/nice-dcv/#does-it-just-worktm","title":"Does it Just Work\u2122?","text":"<p>Like, almost. DCV itself is incredibly stable, but just be warned that if you don't have a valid SSL, you're SOL. Both Jitsi Meet and DCV use TCP/UDP for video streaming (rather than HTTP), and anything Websocket, WebRTC or UDP-based will basically reject a connection unless both sides have valid SSL certificates. SSL certificates are very easily generated from a domain provider like Cloudflare when you own a domain.</p> <p>Also, keep in mind that DCV is closed-source, and early on I tried to figure out how to separate inputs between multiple users in DCV, but I barely made a scratch.</p>"},{"location":"Introduction/open-world-agents/","title":"Open World Agents","text":"<p>PLAICraft, to capture multimodal training data, has a bespoke setup consisting of [[OBS]], a Minecraft mod package called Spigot, and then whatever middleware they used to pack it all in together. Open World Agents is trying to generalize that process for all desktop recording, which makes a ton of sense. It doesn't matter if the user interface is Minecraft, Excel or trading Memecoins; the inputs of screen, audio, mouse and keyboard ought to be the same.</p> <p>Actually, another win for DCV: it allows I/O devices, so bring your USB steering wheel.</p>"},{"location":"Introduction/open-world-agents/#ocap","title":"OCAP","text":"<p>The key component for this project from Open World Agents is called OCAP. OCAP captures full desktop interactions, and then writes it to two files:</p> <ul> <li>MCAP: MCAP is an open-source standard for time-aligned multimodal input files</li> <li>MKV: Just a standard video file type</li> </ul> <p>OCAP mainly relies on an impossibly large library called Gstreamer, which is basically just a ton of multimedia encoders. You can generally install from Gstreamer the same way that people install from a package manager like brew or pip. Open World Agents and OCAP use Anaconda, which as far as I know is a virtual environment and path manager, not unlike a python virtual environment. The main way you call it is via <code>conda</code> and you'll know that you have conda when you see <code>(base)</code> in your terminal.</p>"},{"location":"Introduction/open-world-agents/#does-ocap-just-worktm","title":"Does OCAP Just Work\u2122?","text":"<p>Well... I give them 7.5 / 10. I must accept responsibility as a developer who reads docs too fast (or not at all). But their docs are quite scattered, and they recently broke off OCAP into its own repository which confused me.</p> <p>The biggest issues are critical limitations with OCAP at the time of writing:</p> <ul> <li>It's Windows-only</li> <li>It requires an NVIDIA GPU</li> </ul> <p>This means that you have to:</p> <ul> <li>Request from your cloud provider access to NVIDIA GPUs (I requested 8 vCPUs from AWS, which corresponded to two <code>g4dn.xlarge</code> instances at a time)</li> <li>Spend about 5x more on compute</li> <li>Install the correct NVIDIA drivers on your virtual machine</li> <li>Probably increase the storage while you're at it</li> </ul> <p>I plan on making an open-source contribution to their project, because near the end of my directed study I was able to replace the NVIDIA encoder with a more standard <code>x264enc</code> software encoder. I hope that I can save you some grief. In the event that I was abducted or something, it's not hard. The encoder is hardcoded within the project and OCAP is open-source.</p>"},{"location":"Introduction/open-world-agents/#installation","title":"Installation","text":"<p>The last thing on OCAP for this section: I found that the commands in the docs straight up did not work. Here's what you should do: download the project onto your VM via GitHub, then in Powershell, run this command. This basically installs all the dependencies for the project as far as I know.</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"Introduction/overview/","title":"Introduction","text":"<p>This project is built on several fundamental components that work together to create a scalable cloud framework for capturing training data for desktop agents. Each component plays a crucial role in the overall system.</p>"},{"location":"Introduction/overview/#components-overview","title":"Components Overview","text":"<ul> <li>Jitsi Meet - Video conferencing infrastructure for real-time communication</li> <li>NICE DCV - Remote desktop protocol enabling multi-user access to virtual machines</li> <li>Open World Agents - Desktop interaction capture system using OCAP</li> <li>Windows - Operating system configuration and kiosk mode setup</li> <li>Amazon Web Services - Cloud infrastructure and services</li> </ul> <p>Each of these components has its own challenges and \"gotchas\" that I'll walk you through. Click on any component above to learn more about it, or continue reading below for a high-level overview.</p>"},{"location":"Introduction/overview/#how-they-work-together","title":"How They Work Together","text":"<p>The system architecture leverages these components to create a seamless experience:</p> <ol> <li>Jitsi Meet provides the video conferencing layer for real-time communication</li> <li>NICE DCV enables remote desktop access to powerful cloud workstations</li> <li>Open World Agents (OCAP) captures all desktop interactions in a standardized format</li> <li>Windows Server provides the operating system foundation with kiosk mode for secure, single-app sessions</li> <li>AWS hosts everything in the cloud, providing scalability and reliability</li> </ol> <p>This combination allows you to capture multimodal training data (screen, audio, mouse, keyboard) from any desktop application, whether it's Minecraft, Excel, or trading Memecoins, the inputs remain the same.</p>"},{"location":"Introduction/windows/","title":"Windows","text":"<p>This project will basically turn you into a system administrator. I used Windows Server 2022, which you should understand is not a full-feature Windows. There is no Microsoft Store. There are no AI features. Think of it this way: as an end user who has no computer science background, it's Windows Lite. But for someone looking to experiment with the OS, it should be everything you need.</p>"},{"location":"Introduction/windows/#user-setup","title":"User Setup","text":"<p>The most important part of my workflow on Windows was having two users:</p> <ul> <li>Administrator: Where I wrote code, set up Task Scheduler, messed with registries, etc.</li> <li>KioskUser: The end user</li> </ul>"},{"location":"Introduction/windows/#kiosk-mode","title":"Kiosk Mode","text":"<p>\"Kiosk\" refers to an umbrella term for single-app sessions in Windows. The burger menu screen at your local Wendy's? That's a kiosk.</p> <p>Normally, when a user logs into their desktop on Windows, it runs an executable called <code>explorer.exe</code>, which gives them the desktop GUI. If you replace that with your own executable, like <code>burger.exe</code>, congratulations, you just made a kiosk. The only thing the user has access to (not entirely true, see AppLocker below) is your app. If they <code>Alt + F4</code> and kill that process, they will only get a black screen.</p> <p>The main way that you configure a kiosk (other than creating a brand new user, which is straightforward) is via regedit, or Registry Edit. These are low-level instructions that the OS reads when booting up and when loading specific user profiles.</p> <p>\u26a0\ufe0f Warning: Do not go poking around here without a purpose because you could brick your whole virtual machine.</p>"},{"location":"Introduction/windows/#task-scheduler","title":"Task Scheduler","text":"<p>You'll also get familiar with Task Scheduler, which is a pretty awesome automation application that allows you to basically run any command on a certain trigger. There are pre-written triggers (e.g. at log on), but you can create custom triggers too (I never needed to).</p>"},{"location":"Introduction/windows/#applocker","title":"AppLocker","text":"<p>The last thing I'll mention now is AppLocker. AppLocker basically denies access to certain paths on a desktop, even ones that a user has access to by default. For example, you probably don't want your KioskUser accessing Task Manager, and you can lock that.</p> <p>It is NOT a keyboard logger, so users can still press <code>Ctrl + Alt + Delete</code> and get to the session menu that contains Task Manager.</p>"},{"location":"Phase%201/","title":"Phase 1: Setting up EC2, DCV and OCAP","text":""},{"location":"Phase%201/#overview","title":"Overview","text":"<p>The goal for this section is to use DCV to remote into your virtual machine running Windows, and have run OCAP successfully.</p>"},{"location":"Phase%201/#what-youll-learn","title":"What You'll Learn","text":"<p>This phase covers the complete setup process from creating your EC2 instance to running OCAP:</p> <ol> <li>EC2 - Launch and configure your Windows Server EC2 instance with security groups</li> <li>DCV - Connect via RDP and install NICE DCV for remote desktop</li> <li>OCAP - Install and configure OCAP for desktop capture</li> <li>Fork Discussion - Technical details about the custom OCAP fork</li> </ol> <p>By the end of this phase, you should have:</p> <ul> <li>A running Windows Server 2022 EC2 instance</li> <li>NICE DCV server installed and accessible</li> <li>OCAP successfully installed and running</li> <li>All necessary security groups configured</li> </ul> <p>Let's get started with EC2 Setup \u2192</p>"},{"location":"Phase%201/dcv/","title":"DCV Setup","text":"<p>NICE DCV (Desktop Cloud Visualization) is Amazon's high-performance remote desktop protocol. It's similar to Remote Desktop but optimized for cloud environments with better performance.</p>"},{"location":"Phase%201/dcv/#initial-connection-via-rdp","title":"Initial Connection via RDP","text":"<p>Before we can set up DCV, we need to connect to your Windows EC2 instance using Remote Desktop Protocol (RDP). This allows us to access the full Windows GUI and install software.</p>"},{"location":"Phase%201/dcv/#starting-your-instance","title":"Starting Your Instance","text":"<p>Start your EC2 instance from the EC2 dashboard if it's not already running.</p> <p></p>"},{"location":"Phase%201/dcv/#why-rdp-instead-of-ssh","title":"Why RDP Instead of SSH?","text":"<p>If you're familiar with SSH for Linux instances, you might wonder why we're using RDP. The key difference is that Windows servers have a full GUI, and we need to install software from the internet using a web browser\u2014something that's much easier with a graphical interface.</p> <p>Rather than installing packages via <code>curl</code> or package managers, we'll use the Windows GUI to download and install everything we need.</p>"},{"location":"Phase%201/dcv/#windows-remote-desktop-client","title":"Windows Remote Desktop Client","text":"<p>You'll need the Windows Remote Desktop app to connect. It's available on most operating systems:</p> <ul> <li>Mac: Available in the App Store</li> <li>Windows: Built-in Remote Desktop Connection</li> <li>Linux: Various clients available (Remmina, etc.)</li> </ul> <p></p> <p>Note - Available Everywhere</p> <p>Windows Remote Desktop is available on your OS of choice. This will allow us to remote into our EC2 before we even have DCV installed.</p>"},{"location":"Phase%201/dcv/#getting-your-connection-details","title":"Getting Your Connection Details","text":""},{"location":"Phase%201/dcv/#public-ipv4-address","title":"Public IPv4 Address","text":"<p>When you launch your EC2 instance, AWS automatically provisions a public IPv4 address. This address is assigned on startup and may change if you stop and start the instance.</p> <p></p> <p>Note - Dynamic IP Addresses</p> <p>The public IP address is provisioned automatically on startup and is basically random. We'll have a solution for accessing your EC2 via a static link in the future, but for now, grab the current public IP.</p>"},{"location":"Phase%201/dcv/#getting-rdp-connection-details","title":"Getting RDP Connection Details","text":"<p>Within the EC2 console, click the Connect button for your instance. Select the RDP Client option.</p> <p></p>"},{"location":"Phase%201/dcv/#logging-in","title":"Logging In","text":"<p>If you're using a default Windows AMI (which you should be), AWS will provide you with a password at the bottom of the Connect screen.</p> <p>Use these credentials to log in:</p> <ul> <li>Username: <code>Administrator</code></li> <li>Password: The password provided by AWS (you can change this later in Windows settings)</li> </ul> <p></p>"},{"location":"Phase%201/dcv/#youre-connected","title":"You're Connected!","text":"<p>Once logged in, you should see the Windows desktop.</p> <p></p> <p>Welcome! You are now in your remote computer.</p> <p>Note - Performance Note</p> <p>If the framerate is poor, don't worry about that yet. You might be connected via TCP, which is slower than UDP for video streaming. We'll cover optimizations later (check Phase 6 - Optimizations).</p>"},{"location":"Phase%201/dcv/#installing-dcv-server","title":"Installing DCV Server","text":"<p>Now that you're connected to your Windows instance via RDP, we need to install the DCV Server on your EC2 instance.</p>"},{"location":"Phase%201/dcv/#download-dcv-server","title":"Download DCV Server","text":"<ol> <li>Open Microsoft Edge on your EC2 instance</li> <li>Navigate to: https://www.amazondcv.com/</li> <li>Download the DCV Server for Windows</li> <li>Run the installer with standard settings</li> </ol> <p>Note - Installation Process</p> <p>The setup process is pretty straightforward\u2014similar to installing any Remote Desktop software. If you've used Remote Desktop before, the client will feel familiar.</p>"},{"location":"Phase%201/dcv/#additional-downloads","title":"Additional Downloads","text":"<p>Note - Optional Component</p> <p>There's an additional component you can download from the DCV site. I'm not exactly sure what it does, and I didn't see a huge change after downloading it, but it could help down the road. Feel free to install it if you want.</p>"},{"location":"Phase%201/dcv/#installing-dcv-client-local-machine","title":"Installing DCV Client (Local Machine)","text":"<p>Download and install the DCV client on your local machine (not the EC2 instance). This is the application you'll use to connect to your EC2 instance.</p> <p>The client is available for: - Windows - macOS - Linux - Web browser</p>"},{"location":"Phase%201/dcv/#accessing-dcv","title":"Accessing DCV","text":"<p>There are three ways to access DCV:</p> <ol> <li>Browser Client - Access via web browser (no installation needed)</li> <li>Native Client - The desktop application you just downloaded</li> <li>SDK - Programmatic access (we'll use this later)</li> </ol>"},{"location":"Phase%201/dcv/#browser-access","title":"Browser Access","text":"<p>To access your EC2 via web browser, use your EC2's public DNS address with port 8443:</p> <pre><code>https://ec2-35-88-162-10.us-west-2.compute.amazonaws.com:8443\n</code></pre> <p>Replace the example DNS with your actual EC2 public DNS (found in the EC2 console).</p> <p>Note - Default Port</p> <p>Port 8443 is the default port for DCV connections, which is why we configured it in the security group.</p>"},{"location":"Phase%201/dcv/#what-you-should-have-now","title":"What You Should Have Now","text":"<p>At this point, you should have:</p> <ul> <li>Successfully connected to your EC2 instance via RDP</li> <li>DCV Server installed on your EC2 instance</li> <li>DCV Client installed on your local machine</li> <li>Ability to connect to your EC2 via DCV (browser or native client)</li> </ul> <p>Next: OCAP Installation \u2192</p>"},{"location":"Phase%201/ec2/","title":"EC2 Setup","text":"<p>This guide walks you through setting up a Windows Server EC2 instance that will serve as your remote desktop environment.</p>"},{"location":"Phase%201/ec2/#launching-your-ec2-instance","title":"Launching Your EC2 Instance","text":"<p>Navigate to the EC2 service in the AWS Console and click Launch Instance.</p> <p></p>"},{"location":"Phase%201/ec2/#instance-configuration","title":"Instance Configuration","text":"<p>You have considerable flexibility in how you configure your EC2 instance. Here's what I recommend and why:</p>"},{"location":"Phase%201/ec2/#ami-selection","title":"AMI Selection","text":"<p>Microsoft Windows Server 2022 Base</p> <ul> <li>We need Windows because OCAP requires it</li> <li>Only Windows DCV supports webcam access</li> <li>Use the most recent Windows Server version available</li> </ul>"},{"location":"Phase%201/ec2/#instance-type","title":"Instance Type","text":"<p>t3.xlarge (4 vCPUs, 16 GB RAM)</p> <p>Note - Instance Type Selection</p> <p>The T3 series is like picking Mario in Mario Kart\u2014kinda basic, but you can't go wrong with it. It's the most popular choice for general-purpose workloads.</p> <p>If you're on the free tier, use the largest instance type available to you. You may not even need 16GB RAM for your use case\u2014you can always adjust later once you're comfortable with EC2.</p>"},{"location":"Phase%201/ec2/#key-pair","title":"Key Pair","text":"<p>Create new key pair</p> <p>Note - Key Pairs</p> <p>Key pairs provide an additional layer of security when accessing EC2 instances. You'll create the public/private key pair in your AWS account and keep the private key on your local machine.</p>"},{"location":"Phase%201/ec2/#storage","title":"Storage","text":"<p>Use default settings for now</p> <p>Note - Storage Management</p> <p>You can always add more storage later if needed. I initially added storage for NVIDIA drivers, but it turned out to be unnecessary.</p>"},{"location":"Phase%201/ec2/#security-groups-configuration","title":"Security Groups Configuration","text":"<p>Security groups act as virtual firewalls that control traffic to your EC2 instances. Proper configuration is essential for both security and functionality.</p>"},{"location":"Phase%201/ec2/#security-group-best-practices","title":"Security Group Best Practices","text":"<p>Warning - Avoid This Mistake</p> <p></p> <p>Don't do this! The configuration shown above is messy and not recommended.</p> <p>You should ideally create a separate security group for each purpose. For example, if one EC2 is running your DCV session and another is running a Jitsi server, the ports you need are different.</p>"},{"location":"Phase%201/ec2/#required-security-group-rules","title":"Required Security Group Rules","text":"<p>Here are the ports you'll need to open for this setup:</p> Type Protocol Port Range Source Description RDP TCP <code>3389</code> <code>My IP</code> Initial Install &amp; Backup Access Custom TCP TCP <code>8443</code> <code>0.0.0.0/0</code> NICE DCV (Web/Client) Custom UDP UDP <code>8443</code> <code>0.0.0.0/0</code> NICE DCV (QUIC Support) Custom UDP UDP <code>10000</code> <code>0.0.0.0/0</code> Jitsi Meet (Media/VideoBridge) HTTP TCP <code>80</code> <code>0.0.0.0/0</code> Jitsi Meet (Web Access) HTTPS TCP <code>443</code> <code>0.0.0.0/0</code> Jitsi Meet (Web Access) SSH TCP <code>22</code> <code>My IP</code> SSH Access (Admin) <p>Note - Source IP Configuration</p> <ul> <li>Use <code>My IP</code> for RDP and SSH to restrict access to your current IP address</li> <li>Use <code>0.0.0.0/0</code> for services that need to be publicly accessible (DCV, Jitsi)</li> <li>You can always tighten security later by restricting to specific IP ranges</li> </ul>"},{"location":"Phase%201/ec2/#port-explanations","title":"Port Explanations","text":"<ul> <li>3389 (RDP): Required for initial connection to Windows before DCV is set up</li> <li>8443 (DCV): Default port for NICE DCV connections</li> <li>10000 (Jitsi UDP): Required for Jitsi Meet media streaming</li> <li>80/443 (HTTP/HTTPS): Required for Jitsi Meet web access</li> <li>22 (SSH): Useful for administrative tasks (optional for Windows instances)</li> </ul>"},{"location":"Phase%201/ec2/#launch-the-instance","title":"Launch the Instance","text":"<p>Once you've configured all settings including the security group, click Launch Instance to create your EC2 instance.</p> <p>Next: DCV Setup \u2192</p>"},{"location":"Phase%201/fork-discussion/","title":"Fork Discussion","text":"<p>This document explains the customizations made to the OCAP fork used in this project.</p>"},{"location":"Phase%201/fork-discussion/#repository","title":"Repository","text":"<p>GitHub: https://github.com/shaundano/elephant-ocap</p>"},{"location":"Phase%201/fork-discussion/#why-use-this-fork","title":"Why Use This Fork?","text":"<p>The standard OCAP installation may not work directly on non-GPU EC2 instances. This fork includes several modifications that enable OCAP to run on standard EC2 instances and adds additional features.</p>"},{"location":"Phase%201/fork-discussion/#key-modifications","title":"Key Modifications","text":""},{"location":"Phase%201/fork-discussion/#1-non-gpu-encoder-support","title":"1. Non-GPU Encoder Support","text":"<p>Overview: Swapped the NVIDIA encoder for a standard non-GPU encoder (<code>x264enc</code>). This allows OCAP to run on non-GPU EC2 instances (like t3.xlarge).</p> pipeline.py<pre><code>screen_src |= \"t. ! queue ! d3d11download ! videoconvert ! video/x-raw,format=NV12 ! x264enc tune=zerolatency speed-preset=ultrafast ! h264parse ! queue ! mux.\"\n</code></pre> <p>This line determines the encoder used for video recording.</p>"},{"location":"Phase%201/fork-discussion/#2-microphone-audio-capture","title":"2. Microphone Audio Capture","text":"<p>Overview: Added a second audio capture channel for microphone input. Allows recording both microphone and system audio as separate tracks.</p> pipeline.py<pre><code>if record_mic:\n    src |= ElementFactory.wasapi2src(loopback=False) &gt;&gt; \"audioconvert ! avenc_aac ! queue ! mux.\"\n</code></pre> <p>The <code>wasapi2src</code> component is the same one used for recording system audio. The only difference between input and output audio is the <code>loopback</code> boolean parameter.</p> pipeline.py<pre><code>record_mic: Annotated[bool, typer.Option(help=\"Whether to record microphone input as separate audio track\")] = True,\n</code></pre> <p>The component is passed through setup and during the actual record function.</p>"},{"location":"Phase%201/fork-discussion/#3-process-id-pid-automated-termination","title":"3. Process ID (PID) Automated Termination","text":"<p>Overview: Added PID file writing and graceful termination support. Windows doesn't have a graceful <code>kill</code> command like Unix systems. Sending a <code>task kill</code> to OCAP can corrupt files. This implementation allows graceful termination via SIGINT (Ctrl+C) from external processes.</p> <p>Awesome - PID Termination Implementation</p> <p>This might be the most based part of my project. I absolutely loved getting this to work. It is based on a Stack Overflow discussion. Read the stack overflow kids!</p>"},{"location":"Phase%201/fork-discussion/#writing-the-pid","title":"Writing the PID","text":"recorder.py<pre><code>pid_file = Path(r\"C:\\scripts\\pid\\ocap.pid\")\npid_file.parent.mkdir(parents=True, exist_ok=True)\npid_file.write_text(str(os.getpid()))\n</code></pre> <p>Windows finds its PID and writes it to a static directory (<code>C:\\scripts\\pid\\</code>).</p>"},{"location":"Phase%201/fork-discussion/#reading-and-using-the-pid","title":"Reading and Using the PID","text":"stop-ocap.py<pre><code>try:\n    with open(PID_FILE, \"r\") as f:\n        pid = int(f.read().strip())\n</code></pre> <p>The program reads <code>ocap.pid</code> and gets the process ID.</p>"},{"location":"Phase%201/fork-discussion/#sending-sigint","title":"Sending SIGINT","text":"stop-ocap.py<pre><code>if not kernel32.GenerateConsoleCtrlEvent(CTRL_C_EVENT, 0):\n    print(f\"Failed to send Ctrl+C event. Error: {ctypes.get_last_error()}\")\nelse:\n    print(\"Successfully sent Ctrl+C signal.\")\n</code></pre> <ul> <li><code>kernel32</code> is a Windows system library that provides low-level API functions</li> <li><code>GenerateConsoleCtrlEvent</code> sends the SIGINT signal</li> <li>The SIGINT is defined as Ctrl+C, and <code>0</code> means it targets the entire group of processes attached to the console</li> </ul>"},{"location":"Phase%201/fork-discussion/#cleanup","title":"Cleanup","text":"recorder.py<pre><code>finally:\n    pid_file.unlink(missing_ok=True)\n</code></pre> <p>When OCAP wraps up, it deletes the PID file.</p> <p>Congratulations! You've completed Phase 1. You should now have a fully functional setup with EC2, DCV, and OCAP running.</p>"},{"location":"Phase%201/ocap/","title":"OCAP Installation","text":"<p>OCAP (Open-World Agent Capture) is the desktop interaction capture system we'll use to record screen, audio, and input data. This section covers the installation and configuration process.</p>"},{"location":"Phase%201/ocap/#directory-structure","title":"Directory Structure","text":"<p>First, set up the required directory structure on your EC2 at the <code>C:\\</code> root level:</p> <pre><code>C:\\scripts\\\n\u251c\u2500\u2500 pid\\\n\u2502   \u2514\u2500\u2500 (PID files will be written here - see fork documentation)\n\u251c\u2500\u2500 temp_recordings\\\n\u2502   \u2514\u2500\u2500 (OCAP will write recordings here)\n\u251c\u2500\u2500 meeting\\\n\u2502   \u2514\u2500\u2500 (meeting metadata will go here)\n\u2514\u2500\u2500 (scripts go here)\n\nC:\\projects\\\n\u2514\u2500\u2500 (OCAP repository will go here)\n</code></pre>"},{"location":"Phase%201/ocap/#downloading-the-ocap-fork","title":"Downloading the OCAP Fork","text":"<p>Download the custom OCAP fork as a ZIP file:</p> <p>Repository: https://github.com/shaundano/elephant-ocap</p> <p>Extract the ZIP file into the <code>C:\\projects\\</code> folder.</p> <p>Note - Why Use This Fork?</p> <p>This fork includes several customizations that allow it to run on non-GPU EC2 instances and includes additional features like microphone capture. See the Fork Discussion for technical details.</p>"},{"location":"Phase%201/ocap/#installing-miniconda3","title":"Installing Miniconda3","text":"<p>OCAP requires Conda as a package manager (used by Open World Agents). Install Miniconda3 via PowerShell:</p> Download Miniconda3<pre><code>Invoke-WebRequest -Uri \"https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\" -OutFile \".\\Downloads\\Miniconda3-latest-Windows-x86_64.exe\"\n</code></pre> <p>Run the installer. If given the option to install for all users, accept it.</p> <p>Installation Complete</p> <p>Once installed, you should see <code>(base)</code> in your PowerShell prompt, indicating that Conda is active.</p>"},{"location":"Phase%201/ocap/#creating-the-conda-environment","title":"Creating the Conda Environment","text":"<p>Create a new Conda environment for OCAP:</p> Create Conda Environment<pre><code>conda create -n ocap-env python=3.14\n</code></pre> <p>Activate the environment:</p> Activate Conda Environment<pre><code>conda activate ocap-env\n</code></pre> <p>You should now see <code>(ocap-env)</code> in your terminal prompt.</p>"},{"location":"Phase%201/ocap/#installing-ocap","title":"Installing OCAP","text":"<p>Navigate to the OCAP project folder in PowerShell, then install it in editable mode:</p> Install OCAP<pre><code>pip install -e .\n</code></pre> <p>Note - Editable Mode</p> <p>The <code>-e</code> flag installs in editable mode, which is like debug mode or hot refresh. If dependencies change in the Python project, it should automatically react. The dependencies are defined in <code>pyproject.toml</code>.</p>"},{"location":"Phase%201/ocap/#installing-gstreamer-dependencies","title":"Installing GStreamer Dependencies","text":"<p>Follow the official OCAP documentation and install GStreamer dependencies:</p> Install GStreamer Dependencies<pre><code># Install GStreamer dependencies first (for video recording)\nconda install open-world-agents::gstreamer-bundle\n</code></pre> <p>The regular documentation says to also run <code>pip install ocap</code>, but since we downloaded it locally, we don't need to do that.</p>"},{"location":"Phase%201/ocap/#running-ocap","title":"Running OCAP","text":"<p>You should now be ready to run OCAP. Execute:</p> Run OCAP<pre><code>ocap my-recording\n</code></pre> <p>This will write files to the default path, which is <code>C:\\</code>.</p>"},{"location":"Phase%201/ocap/#success-indicators","title":"Success Indicators","text":"<p>If everything is working correctly, you should see output similar to:</p> <p> </p>"},{"location":"Phase%201/ocap/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Phase%201/ocap/#common-issues","title":"Common Issues","text":"<p>Warning - Don't Give Up!</p> <p>Getting OCAP to run was one of the hardest parts of this project to debug. Once it worked, I didn't even really understand why it started working.</p>"},{"location":"Phase%201/ocap/#environment-activation","title":"Environment Activation","text":"<p>Make sure your Conda environment is activated. You should see <code>(ocap-env)</code> on your terminal line.</p>"},{"location":"Phase%201/ocap/#missing-gstreamer-components","title":"Missing GStreamer Components","text":"<p>Error: <code>no module 'gi' found</code></p> <p>OCAP uses many different GStreamer components, and sometimes they'll be missing. Try:</p> <ol> <li>Re-installing the GStreamer plugins from open-world-agents</li> <li>Adding the <code>conda-forge</code> channel (should be enabled by default, but sometimes it isn't):</li> </ol> Add Conda Forge Channel<pre><code>conda config --add channels conda-forge\n</code></pre>"},{"location":"Phase%201/ocap/#why-use-the-fork-instead-of-pip-install","title":"Why Use the Fork Instead of pip install?","text":"<p>By downloading the project directly onto your EC2, you're able to rely on your own local repository rather than <code>pip install ocap</code>, which uses the most recent version. This gives you visibility into the dependencies you need in <code>pyproject.toml</code> and allows you to customize the code if needed.</p> <p>Next: Fork Discussion \u2192</p>"},{"location":"Phase%202/","title":"Phase 2: OCAP Automation and S3","text":""},{"location":"Phase%202/#overview","title":"Overview","text":"<p>In this section, we're going to write some scripts building off OCAP, plus introduce the python scripts to handle the stopping of OCAP and the upload to S3.</p>"},{"location":"Phase%202/#what-youll-learn","title":"What You'll Learn","text":"<p>This phase covers automating OCAP and setting up cloud storage:</p> <ol> <li>S3 Setup - Create and configure an S3 bucket for storing recordings</li> <li>IAM Roles and AWS Tools - Configure IAM permissions and install AWS tools</li> <li>OCAP Scripts - Create scripts for running, stopping, and uploading OCAP recordings</li> <li>Task Scheduler - Automate OCAP to start on unlock and stop/upload on lock</li> </ol>"},{"location":"Phase%202/#expected-outcome","title":"Expected Outcome","text":"<p>By the end of this phase, you should have:</p> <ul> <li>An S3 bucket configured for storing OCAP recordings</li> <li>IAM roles configured for EC2 to access S3</li> <li>Scripts to run, stop, and upload OCAP recordings</li> <li>Task Scheduler automation to trigger OCAP on workstation unlock/lock</li> </ul> <p>Let's get started with S3 Setup \u2192</p>"},{"location":"Phase%202/iam-aws-tools/","title":"IAM Roles and AWS Tools","text":"<p>IAM permissions will allow our EC2 to execute commands affecting our S3 bucket. The issue is that my IAM role does a whole lot, and I don't want to jump the gun, but it's probably best for you to use my full IAM role now. Trust me bro.</p>"},{"location":"Phase%202/iam-aws-tools/#creating-the-iam-role","title":"Creating the IAM Role","text":"<p>Go to the IAM AWS service, and create a new role. Follow this config:</p> <ul> <li><code>AWS Service</code> trusted entity type</li> <li><code>EC2</code> Use Case</li> <li>Select <code>AmazonS3FullAccess</code> as a default service. These are just pre-built policies.</li> <li>Create the role.</li> </ul>"},{"location":"Phase%202/iam-aws-tools/#adding-inline-policies","title":"Adding Inline Policies","text":"<p>Now in the list of all the roles, click your role. Go to Add permissions and select <code>Create inline policy</code>. This allows you to paste in a JSON.</p>"},{"location":"Phase%202/iam-aws-tools/#ocaps3writepolicy","title":"OcapS3WritePolicy","text":"<p>The first policy is called <code>OcapS3WritePolicy</code>. Here it is:</p> OcapS3WritePolicy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowListBucket\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\"\n            ],\n            \"Resource\": \"arn:aws:s3:::elephant-bucket-ocap-recordings\"\n        },\n        {\n            \"Sid\": \"AllowOcapReadWriteToSpecificBucket\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListMultipartUploadParts\",\n                \"s3:AbortMultipartUpload\"\n            ],\n            \"Resource\": \"arn:aws:s3:::elephant-bucket-ocap-recordings/*\"\n        }\n    ]\n}\n</code></pre> <p>This basically allows upload to S3, including multi-part uploading. I'm not exactly sure the use case for that, but I assume that if a file is too big, it will upload it in chunks. I'm not sure the threshold, but it's good to have.</p> <p>Note - Update Bucket Name</p> <p>Replace <code>elephant-bucket-ocap-recordings</code> with your actual bucket name in the policy above.</p>"},{"location":"Phase%202/iam-aws-tools/#dynamodb-read-policy","title":"DynamoDB Read Policy","text":"<p>This next role is called <code>all-dynamo-db-read</code>. It just provides full permissions for an EC2 to read a DynamoDB table. Again, my roles are kind of a mess, if you aren't in a crunch you can probably be a bit more elegant.</p> all-dynamo-db-read<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:DescribeImport\",\n                \"dynamodb:DescribeContributorInsights\",\n                \"dynamodb:ListTagsOfResource\",\n                \"dynamodb:*\",\n                \"dynamodb:GetAbacStatus\",\n                \"dynamodb:DescribeReservedCapacityOfferings\",\n                \"dynamodb:PartiQLSelect\",\n                \"dynamodb:DescribeTable\",\n                \"dynamodb:GetItem\",\n                \"dynamodb:DescribeContinuousBackups\",\n                \"dynamodb:DescribeExport\",\n                \"dynamodb:GetResourcePolicy\",\n                \"dynamodb:DescribeKinesisStreamingDestination\",\n                \"dynamodb:DescribeLimits\",\n                \"dynamodb:BatchGetItem\",\n                \"dynamodb:ConditionCheckItem\",\n                \"dynamodb:Scan\",\n                \"dynamodb:Query\",\n                \"dynamodb:DescribeStream\",\n                \"dynamodb:DescribeTimeToLive\",\n                \"dynamodb:ListStreams\",\n                \"dynamodb:DescribeGlobalTableSettings\",\n                \"dynamodb:GetShardIterator\",\n                \"dynamodb:DescribeGlobalTable\",\n                \"dynamodb:DescribeReservedCapacity\",\n                \"dynamodb:DescribeBackup\",\n                \"dynamodb:DescribeEndpoints\",\n                \"dynamodb:GetRecords\",\n                \"dynamodb:DescribeTableReplicaAutoScaling\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>Cool. You should have three policies in your role.</p> <p></p> <p>This is my IAM permission. Note that \"ReadOnly-Role\" is a misnomer. This bad boy can 100% write stuff. Don't blame me, there's no <code>edit name</code> button and I can't be bothered to look for one.</p>"},{"location":"Phase%202/iam-aws-tools/#attaching-the-role-to-ec2","title":"Attaching the Role to EC2","text":"<p>Attach your role to your EC2.</p> <p></p> <p>Now, AWS will allow your EC2 to read and write to DynamoDB and S3, but it doesn't know it yet. Now we need to enable AWS tools on the EC2 itself.</p>"},{"location":"Phase%202/iam-aws-tools/#enabling-aws-tools","title":"Enabling AWS Tools","text":"<p>You need to install AWS tools for PowerShell on the EC2.</p> Install AWS PowerShell Module<pre><code>Install-Module -Name AWSPowerShell -Force -AllowClobber\n</code></pre> <p>Run this to be sure it worked:</p> Verify Installation<pre><code>Get-Module -ListAvailable -Name AWSPowerShell\n</code></pre> <p>You should see something like:</p> <pre><code>ModuleType Version    Name\n---------- -------    ----\nBinary     4.1.892    AWSPowerShell\n</code></pre> <p>Note - AWS Tools vs boto3</p> <p>You might not need these AWS tools, because we're about to use a python library called <code>boto3</code> which is extremely standard for python to interact with AWS services. But just in case you're going to be executing AWS commands from PowerShell, this is good to have.</p> <p>Next: OCAP Scripts \u2192</p>"},{"location":"Phase%202/ocap-scripts/","title":"OCAP Scripts","text":"<p>Now we're going to set up OCAP such that upon workstation unlock, it will run, and then upon lock (when the user leaves the DCV session), it will stop the OCAP script and upload to your S3 bucket.</p>"},{"location":"Phase%202/ocap-scripts/#creating-the-ocap-start-script","title":"Creating the OCAP Start Script","text":"<p>Go back to your EC2. And go to the Notepad. You're going to write the following script:</p> ocap.ps1<pre><code># 1. Activate your environment\nconda activate ocap-env\n\n# 2. Create the timestamped name\n$timestamp = Get-Date -Format \"yyyyMMdd-HHmmss\"\n$baseName = \"session_$($timestamp)\"\n\n# 3. Create the full path in your required folder\n$fullPath = Join-Path -Path \"C:\\scripts\\temp_recordings\" -ChildPath $baseName\n\n# 4. Run the ocap command with the full path\nocap $fullPath\n</code></pre> <p>Then save this as <code>ocap.ps1</code>, and ensure that you save as \"All files\". Save it in your <code>scripts</code> folder at the <code>C:\\</code> level. This is a PowerShell script that, when run, will run OCAP and write to your temp_recordings folder.</p>"},{"location":"Phase%202/ocap-scripts/#how-to-test","title":"How to Test","text":"<ul> <li>Activate your conda environment, <code>ocap-env</code> in PowerShell</li> <li>Navigate to <code>C:/scripts</code></li> <li>Run <code>./ocap</code></li> </ul> <p>This should work.</p>"},{"location":"Phase%202/ocap-scripts/#creating-the-stop-script","title":"Creating the Stop Script","text":"<p>The stop_ocap script is a Python script that looks for the PID written by the OCAP program and sends a SIGINT (if any of this is confusing to you, go back and read Fork Discussion in Phase 1).</p> <p>I'm not going to provide the full stop script here (it should be in the repo), and I explain it in Fork Discussion. I will just go through some quick points.</p> stop_ocap.py<pre><code># --- 1. Set the correct path to your PID file ---\nPID_FILE = r\"C:\\scripts\\pid\\ocap.pid\"\n\n# --- 2. Define Windows API functions ---\nkernel32 = ctypes.windll.kernel32\nCTRL_C_EVENT = 0\n</code></pre> <ul> <li>Finds the PID file</li> <li>Kernel32 is a low-level Windows API</li> <li>CTRL_C_EVENT is a static local variable</li> </ul> stop_ocap.py<pre><code>try:\n    with open(PID_FILE, \"r\") as f:\n        pid = int(f.read().strip())\n</code></pre> <ul> <li>Program finds the PID</li> </ul> stop_ocap.py<pre><code>kernel32.FreeConsole()\nkernel32.AttachConsole(pid)\nkernel32.SetConsoleCtrlHandler(None, True)\nkernel32.GenerateConsoleCtrlEvent(CTRL_C_EVENT, 0)\nkernel32.FreeConsole()\nkernel32.SetConsoleCtrlHandler(None, False)\n</code></pre> <ul> <li>Detach from this program's console</li> <li>Attach to the PID's console</li> <li>Disable Ctrl + C in this process. Even though we detached, AttachConsole(pid) adds current process to the pid's console which still opens us up</li> <li>Generate Ctrl + C event and send to all processes in the console</li> <li>Detach from PID's console</li> <li>Re-enable Ctrl + C on this process</li> </ul>"},{"location":"Phase%202/ocap-scripts/#creating-the-upload-script","title":"Creating the Upload Script","text":"<p>In <code>C:\\scripts</code>, you're going to add the <code>upload_to_s3.py</code> script. Now, again this script is in the repo, but I will describe a few of the lines.</p> upload_to_s3.py<pre><code>import boto3\n</code></pre> <p>You will probably need to install boto3 into your conda environment. Just run:</p> Install boto3<pre><code>pip install boto3\n</code></pre> upload_to_s3.py<pre><code>RECORDING_FOLDER = r\"C:\\scripts\\temp_recordings\"\nS3_BUCKET = \"elephant-bucket-ocap-recordings\"\nAWS_REGION = \"us-west-2\" # Match the region used in get_meeting_link.py\n</code></pre> <ul> <li>Sets static local variables for where to find the recordings, the S3 bucket name, and the region it's located in.</li> </ul> upload_to_s3.py<pre><code>mcap_files = glob.glob(os.path.join(RECORDING_FOLDER, \"*.mcap\"))\nmkv_files = glob.glob(os.path.join(RECORDING_FOLDER, \"*.mkv\"))\nfiles_to_upload = mcap_files + mkv_files\n\n# Construct the public S3 URL (using virtual-hosted style)\ns3_url = f\"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{s3_key}\"\ns3_client.upload_file(archive_path, S3_BUCKET, s3_key)\n</code></pre> <ul> <li>Finds the files to upload and uploads them</li> </ul>"},{"location":"Phase%202/ocap-scripts/#testing-all-scripts","title":"Testing All Scripts","text":"<p>Try all three steps now: run OCAP, stop it via <code>stop_ocap.py</code> and then upload the data via <code>upload_to_s3.py</code>. If this does not work, there are a ton of debug lines within the scripts themselves, hopefully that helps. This part shouldn't be that bad, however.</p> <p>Put <code>stop_ocap.py</code> at the same level as your <code>ocap.ps1</code> PowerShell script. Run OCAP the same way you did above. You should see the pid file get written in <code>scripts/pid</code>. Let OCAP run for a few seconds, and then run:</p> Test stop_ocap.py<pre><code>python ./stop_ocap.py\n</code></pre> <p>You should see your OCAP script get terminated gracefully. You'll know that it terminated gracefully if in <code>temp_recordings</code>, both your MCAP and MKV videos will not be 0 B large. As a rough rule, whatever MCAP is in KB, MKV will be in MB.</p> <p>Note - Script Dependencies</p> <p>Keep in mind that the scripts in the repo include steps that we haven't completed yet: we haven't even touched DynamoDB. So if you're getting errors related to stuff we haven't set up yet, just comment out those lines; I tried to make things reasonably modular.</p> <p>Next: Task Scheduler \u2192</p>"},{"location":"Phase%202/s3-setup/","title":"S3 Setup","text":"<p>The first thing we're going to do is set up the S3 bucket and handle the installation of AWS tools on the EC2 instance. The reason you need to do this is because AWS tools allows EC2 instances to access AWS services. This is also where we're going to define the IAM role for the EC2.</p>"},{"location":"Phase%202/s3-setup/#creating-an-s3-bucket","title":"Creating an S3 Bucket","text":"<p>First, navigate to the S3 service in AWS. Then, hit Create bucket. And then... you literally just give it a name and hit create. It Just Works <sup>TM</sup>.</p> <p></p> <p>You now have an S3 bucket that can store files of any type.</p>"},{"location":"Phase%202/s3-setup/#configuring-public-access","title":"Configuring Public Access","text":"<p>Although as an AWS user you have a GUI where you can navigate your files, the public does not. If you want your files accessible by members of the public (including Frank iykyk), go to the Permissions tab, and uncheck ALL the boxes related to <code>Block public access</code>.</p>"},{"location":"Phase%202/s3-setup/#bucket-policy","title":"Bucket Policy","text":"<p>Then, you're going to write a Bucket policy. Note that pretty much all Permissions policies in AWS are JSON objects. Here's the bucket policy to use for the public to have access to your S3:</p> Bucket Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObjectAndList\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::elephant-bucket-ocap-recordings\",\n                \"arn:aws:s3:::elephant-bucket-ocap-recordings/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Note - Update Bucket Name</p> <p>Replace <code>elephant-bucket-ocap-recordings</code> with your actual bucket name in the policy above.</p> <p>This should be enough to make it available. You'll be able to test it later, but the idea is that if you press \"Copy URL\", you should just be able to copy that into someone's browser and have them download a file, like so:</p> <pre><code>https://elephant-bucket-ocap-recordings.s3.us-west-2.amazonaws.com/123_teacher_20251201T051833.zip\n</code></pre> <p>Next: IAM Roles and AWS Tools \u2192</p>"},{"location":"Phase%202/task-scheduler/","title":"Task Scheduler","text":"<p>I'm gonna just say this right now: Task Scheduler is pretty easy to set up, however the settings are quite important. If you are too liberal with how you set this up, there's a strong chance something will not work.</p>"},{"location":"Phase%202/task-scheduler/#setting-up-task-1-start-ocap","title":"Setting Up Task 1: Start OCAP","text":"<p>Open Task Scheduler on your EC2. We're going to set up two tasks. The first one is going to Start OCAP. Go to the right hand side in Task Scheduler, and select \"Create Task\". You should be staring at a screen like this:</p> <p></p>"},{"location":"Phase%202/task-scheduler/#configuring-the-trigger","title":"Configuring the Trigger","text":"<p>First off, go to \"Triggers\". This will decide how our task will be... triggered. Yeah that might have been obvious. Here's the config for the Trigger:</p> <ul> <li>Begin the task: On workstation unlock<ul> <li>Fun fact: there's a whole AWS docs page about Task Scheduler and DCV which as far as I know is WRONG. You might see \"connect to user session\" as the answer, and if that works for you, mazel tov. This is what I did.</li> </ul> </li> <li>Any User for now (we haven't configured our KioskUser yet)</li> <li>CHECK OFF ENABLED</li> </ul>"},{"location":"Phase%202/task-scheduler/#configuring-the-action","title":"Configuring the Action","text":"<p>Now go to Actions. Here's the Config:</p> <ul> <li>Start a program</li> <li>Program/script: <code>powershell.exe</code></li> <li>Arguments: <code>C:\\scripts\\ocap.ps1</code></li> </ul>"},{"location":"Phase%202/task-scheduler/#important-configuration-settings","title":"Important Configuration Settings","text":"<p>This is important. On the first page, this is the config you need:</p> <ul> <li>When running task, use the following user account: Administrator for now<ul> <li>Basically, whatever user is meant to run this, select that user.</li> </ul> </li> <li>Select Run only when user is logged on</li> <li>Check Run with highest privileges</li> <li>Configure for Windows Server 2022</li> </ul> <p>Warning - Session Context is Critical</p> <p>If you don't do any of this, Windows won't run OCAP inside the session; it will run it in a totally separate session. This is not always a bad thing; but basically the user's session is session 1 to Windows, and there is a higher plane of existence called session 0. The issue is that we are trying to capture the user's inputs, and we NEED to run it in this session.</p> <p>And that's the first task. Now we create the second. I'm gonna speed through this a bit.</p>"},{"location":"Phase%202/task-scheduler/#setting-up-task-2-stop-ocap-and-upload","title":"Setting Up Task 2: Stop OCAP and Upload","text":"<p>Name: Stop OCAP</p>"},{"location":"Phase%202/task-scheduler/#trigger-configuration","title":"Trigger Configuration","text":"<ul> <li>Trigger: On Workstation lock</li> <li>Any User for now</li> <li>Enabled</li> </ul>"},{"location":"Phase%202/task-scheduler/#actions-configuration","title":"Actions Configuration","text":"<p>This one has two actions: first, the stop script, then the upload script.</p> <p>Action 1: Start a program - Program/script: <code>C:\\Users\\Administrator\\Miniconda3\\envs\\ocap-env\\python.exe</code> - Arguments: <code>C:\\scripts\\stop_ocap.py</code></p> <p>Action 2: Start a program - Program/script: <code>C:\\Users\\Administrator\\Miniconda3\\envs\\ocap-env\\python.exe</code> - Arguments: <code>C:\\scripts\\upload_to_s3.py</code></p> <p>Note - Action Execution Order</p> <p>Note that these do not necessarily happen synchronously, which is why I put a sleep at the beginning of my upload script.</p>"},{"location":"Phase%202/task-scheduler/#testing","title":"Testing","text":"<p>Try this whole setup. You can reboot your EC2 instance if you want things to take effect. Also, if you open <code>services.msc</code> in the Run menu, you can find the DCV server and restart it for changes to take effect.</p> <p>You will know that you were successful if:</p> <ol> <li>You log in and you see OCAP boot up</li> <li>Once OCAP is running, you exit out of your DCV viewer, and then you see your S3 bucket get populated by the files</li> </ol>"},{"location":"Phase%202/task-scheduler/#troubleshooting","title":"Troubleshooting","text":"<p>If it's not working, the first thing you should do is enable history in Task Scheduler for your tasks. You will see if there were any errors. Some of the errors are just numbers (8000x), just look those up. Also, on booting up, if it doesn't work, just try logging in again. We'll deal with that later, but I think that the trigger for \"first\" log in after booting up is treated differently by Windows.</p> <p>Congratulations! You've completed Phase 2. You should now have automated OCAP recording that starts on workstation unlock and stops/uploads to S3 on workstation lock.</p>"},{"location":"Phase%203/","title":"Phase 3: First Lambdas, DynamoDB, and Frontend","text":""},{"location":"Phase%203/#overview","title":"Overview","text":"<p>In this phase, we're going to set up API Gateway for the first time, which will establish API endpoints that we can hit from the frontend. These endpoints will call Lambda functions that modify our DynamoDB. If this sounds like gibberish to you, go back to the Introduction where we discuss various AWS services present in this project.</p>"},{"location":"Phase%203/#what-youll-learn","title":"What You'll Learn","text":"<p>This phase covers setting up the backend infrastructure and frontend integration:</p> <ol> <li>DynamoDB - Create and configure the DynamoDB table for storing meeting data</li> <li>Frontend - Set up the frontend interface and understand the data flow</li> <li>Schedule-meeting Lambda - Create the Lambda function to handle meeting scheduling</li> <li>API Gateway and Testing - Set up API Gateway endpoints and test with CloudWatch</li> </ol>"},{"location":"Phase%203/#expected-outcome","title":"Expected Outcome","text":"<p>By the end of this phase, you should have:</p> <ul> <li>A DynamoDB table configured for storing meeting data</li> <li>A frontend interface for scheduling meetings</li> <li>A Lambda function that processes meeting requests</li> <li>API Gateway endpoints connected to your Lambda</li> <li>Ability to test and monitor Lambda executions via CloudWatch</li> </ul> <p>Let's get started with DynamoDB \u2192</p>"},{"location":"Phase%203/api-gateway/","title":"API Gateway and Testing","text":"<p>Navigate to API Gateway in AWS, and create a new one. Then select REST API.</p> <p></p>"},{"location":"Phase%203/api-gateway/#api-configuration","title":"API Configuration","text":"<p>Configure it like this:</p> <ul> <li>Name it <code>schedule</code></li> <li>Go with any recommended security policy</li> <li>Keep everything else default</li> </ul> <p></p>"},{"location":"Phase%203/api-gateway/#creating-the-resource","title":"Creating the Resource","text":"<p>Now we need to create the actual endpoint (resource):</p> <ol> <li>In your API dashboard, click Actions &gt; Create Resource.</li> <li>Resource Name: Enter <code>schedule</code>.</li> <li>Resource Path: This should auto-fill as <code>/schedule</code>.</li> <li>Check CORS.</li> </ol> <p>Note - CORS (Cross-Origin Resource Sharing)</p> <p>CORS is a browser security feature that blocks web pages from making API requests to a different domain. You 100% need it if you're calling the API from a frontend.</p> <ol> <li>Click Create Resource.</li> </ol>"},{"location":"Phase%203/api-gateway/#creating-the-post-method","title":"Creating the POST Method","text":"<p>Next, you need to create the POST method with this config:</p> <ul> <li>Method type: POST</li> <li>Integration type: Lambda</li> <li>Lambda function: your region, and find the ARN for your Lambda</li> </ul> <p>Note - ARN (Amazon Resource Name)</p> <p>ARN is your service's unique ID across AWS services.</p> <p>Once you've set that up, ensure that you've enabled CORS one more time. Just select the POST method in the menu and enable CORS in the top right. Then deploy the API. You'll have to probably define the environment you're deploying in, just call it <code>prod</code>.</p>"},{"location":"Phase%203/api-gateway/#api-gateway-url","title":"API Gateway URL","text":"<p>Your API Gateway URL will follow this format:</p> <pre><code>https://{gateway-id}.execute-api.us-west-2.amazonaws.com/prod\n</code></pre> <p>Now if you replace <code>API_GATEWAY_URL</code> with your URL in this way, you'll be able to hit the POST method.</p> <p>That's it. You should be able to schedule a meeting via the frontend, have it write to the database, and have an EventBridge scheduled task invoke the launch-ec2 Lambda function. Although we haven't set that one up yet, here's how you can test Lambdas.</p>"},{"location":"Phase%203/api-gateway/#testing-with-cloudwatch","title":"Testing with CloudWatch","text":"<ul> <li>Go to Monitor, and then View CloudWatch logs. Every execution of the Lambda should have a log, and any print statements, plus built-in logs will display there.</li> </ul> <p>Testing - CloudWatch Logs</p> <p>If successful, you should see a line like:</p> <pre><code>Scheduling Launch for: 2025-11-30T22:23:00\n</code></pre> <p>Congratulations! You've completed Phase 3. You should now have a working API Gateway endpoint connected to your Lambda function, and be able to schedule meetings through the frontend.</p>"},{"location":"Phase%203/dynamodb/","title":"DynamoDB","text":"<p>DynamoDB is super easy to set up, it's about as easy as S3. We're going to have quite a few fields in ours.</p>"},{"location":"Phase%203/dynamodb/#creating-the-table","title":"Creating the Table","text":"<p>Go over to DynamoDB and just create a table. The only thing you should know about if you're new to databases is the Primary Key. Primary keys are unique identifiers for a row within the database.</p> <p>Note - Primary Keys and Foreign Keys</p> <p>If a primary key for a given row is present in another table, this is called a foreign key. This is how relational databases work; if you have a customer table and a payment method table, you would put the primary key for the payment method row in customer table, which guarantees that the payment method in the customer table will have a corresponding item in the primary key table. This is all useless here since we're only using one table, I just like to yap.</p>"},{"location":"Phase%203/dynamodb/#dynamic-schema","title":"Dynamic Schema","text":"<p>Awesome - Dynamic Columns</p> <p>The cool thing about DynamoDB is that columns will just show up if you write to a column. Like if we send an object with \"first_name and last_name\", we will get those columns automatically. Pretty awesome.</p> <p>We don't really need to do anything else. Before writing the actual function that writes to our database, we're going to switch gears into frontend and look at the actual data we're going to send when we schedule a meeting.</p> <p>Next: Frontend \u2192</p>"},{"location":"Phase%203/frontend/","title":"Frontend","text":"<p>The first thing you should do is access the frontend wireframe via the repository. The way that I run things locally is using Live Server, which is a really useful plugin for VSCode when running anything static; vanilla JS and HTML are perfect for this.</p> <p> </p> <p><code>index.html</code> is the entry point for my simple frontend. If you launch with Live Server, you should see this:</p> <p></p> <p>Note - Frontend Simplicity</p> <p>Note that my frontend is super basic and mostly AI-generated. In a production environment, you would probably integrate 3rd party components, would make all the underlying API calls. Therefore the HTTP requests are the most important part.</p>"},{"location":"Phase%203/frontend/#key-code-components","title":"Key Code Components","text":"<p>These are the two most important chunks in <code>scheduleForm.js</code>:</p>"},{"location":"Phase%203/frontend/#payload-definition","title":"Payload Definition","text":"<pre><code>const payload = {\n    teacher_name: teacherName,\n    student_name: studentName,\n    meet_time: convertToISOString(meetTime, timezone),\n    frontend_base_url: frontendBaseUrl\n};\n</code></pre> <ul> <li>This defines the payload that we're going to send to our API Gateway.</li> </ul>"},{"location":"Phase%203/frontend/#http-request","title":"HTTP Request","text":"<pre><code>const response = await fetch(API_GATEWAY_URL, {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json',\n    },\n    body: JSON.stringify(payload)\n});\n</code></pre> <ul> <li>This is the actual request. Note that it is a POST request, meaning that it actually has a payload.</li> <li><code>API_GATEWAY_URL</code> is a static variable that I define in my <code>.env</code> file.</li> </ul> <p>Warning - API Security</p> <p>I think that you should probably keep your API URLs hidden. Normally, you can have a secret key for accessing an API, but I don't have one (lol). I may have defined one in AWS, but if that is the case, I'm not exactly sure how that would vet local traffic. The only thing I can imagine is that with a security group, you can limit which IP addresses get through. Regardless, I'm not gonna share my URL and you should probably be more secure than I was.</p> <p>At this point, you don't have an API URL, so the frontend won't do anything. We're going to set up API Gateway so that we have an endpoint to hit, but first we need to write our schedule meeting Lambda. We're going to write it pretending we already have a way to send this whole payload to it.</p> <p>Next: Schedule-meeting Lambda \u2192</p>"},{"location":"Phase%203/schedule-meeting-lambda/","title":"Schedule-meeting Lambda","text":"<p>When you create your first Lambda, the only thing you should ensure is that your runtime is Python. I went with Python 3.14.</p>"},{"location":"Phase%203/schedule-meeting-lambda/#key-imports","title":"Key Imports","text":"<p>The most important import for our Lambda is <code>boto3</code>. This allows us to perform read / write operations with other AWS services.</p>"},{"location":"Phase%203/schedule-meeting-lambda/#critical-code-components","title":"Critical Code Components","text":"<p>Here are the critical pieces of this Lambda:</p>"},{"location":"Phase%203/schedule-meeting-lambda/#initial-setup","title":"Initial Setup","text":"Initial Setup<pre><code>dynamodb = boto3.resource('dynamodb')\nscheduler = boto3.client('scheduler')\n\nTABLE_NAME = 'elephant-meetings'\ntable = dynamodb.Table(TABLE_NAME)\nJITSI_DOMAIN = 'meet.christardy.com'\n</code></pre> <ul> <li><code>elephant-meetings</code> is the name of my DynamoDB table</li> <li><code>scheduler</code> refers to EventBridge Scheduler. More on that in a second.</li> <li><code>JITSI_DOMAIN</code> refers to the domain that I selected to host my Jitsi server on.</li> </ul>"},{"location":"Phase%203/schedule-meeting-lambda/#meeting-id-and-jitsi-url-generation","title":"Meeting ID and Jitsi URL Generation","text":"Meeting ID and Jitsi URL Generation<pre><code>meeting_id = generate_id()\njitsi_url = f\"https://{JITSI_DOMAIN}/{meeting_id}\"\n</code></pre> <p>Awesome - Jitsi Meeting Provisioning</p> <p>Remember that Jitsi provisions meetings on the fly. The way I decided to do this is to just use the random primary key from DynamoDB as the meeting room name. That ensures that there won't be any collisions and I don't need to actually talk to the Jitsi service at all to get a meeting room.</p>"},{"location":"Phase%203/schedule-meeting-lambda/#eventbridge-scheduler","title":"EventBridge Scheduler","text":"EventBridge Scheduler<pre><code>launch_dt = meet_dt - timedelta(minutes=10) \nlaunch_iso = launch_dt.strftime('%Y-%m-%dT%H:%M:%S')\n\nprint(f\"Scheduling Launch for: {launch_iso}\")\n\nscheduler.create_schedule(\n    Name=f\"launch-{meeting_id}\",\n    ScheduleExpression=f\"at({launch_iso})\",\n    Target={\n        'Arn': launcher_arn,\n        'RoleArn': scheduler_role,\n        'Input': json.dumps({'meeting_id': meeting_id})\n    },\n    FlexibleTimeWindow={'Mode': 'OFF'},\n    ActionAfterCompletion='DELETE'\n)\n</code></pre> <p>Note - EventBridge Scheduler</p> <p>This is a good time to talk about EventBridge Scheduler. It's a really intelligent AWS service that you can call directly from boto3. It has a GUI just like any other service in AWS, but we are instead calling it programmatically.</p> <p>What it does is you can set a timed trigger independent of the Lambda, and have it execute specific behaviour. What I use it for is if a Lambda should call another Lambda, you can use EventBridge.</p> <p>In this case, the payload from the frontend has a meeting time, which is when the teacher has scheduled the tutoring session. <code>launch_dt</code> is defined as that meeting time minus 10 minutes (EC2's need time to spin up, and we will be running our own health checks).</p> <p>Here are a few more details about the EventBridge object we're creating:</p> <ul> <li><code>Name</code>: Assigns a unique identifier to this schedule using the <code>meeting_id</code>.</li> <li><code>ScheduleExpression</code>: Sets the exact execution time (<code>at(...)</code>) using an ISO timestamp.</li> <li><code>Target</code>: Defines what to trigger (the <code>launcher_arn</code>), the permissions to do so (<code>RoleArn</code>), and the JSON payload (<code>Input</code>) to send.</li> <li><code>FlexibleTimeWindow</code>: <code>OFF</code> ensures that Amazon executes it at the exact time specified.</li> <li><code>ActionAfterCompletion</code>: Automatically deletes the schedule entry after it runs once.</li> </ul> <p>Testing - EventBridge Scheduler</p> <p>Something that is useful for testing: if you schedule a meeting in the past, EventBridge scheduler will trigger right away.</p> <p>Note - Safety Net Scheduler</p> <p>There's another scheduler that handles the meeting safety net, which will be covered later in these docs, but in short, if no one shows up to the scheduled meeting after 15 minutes, it will kill the EC2 instances provisioned so that you don't rack up a huge AWS bill.</p>"},{"location":"Phase%203/schedule-meeting-lambda/#dynamodb-payload","title":"DynamoDB Payload","text":"<p>Here's the payload in <code>schedule-meeting</code>:</p> DynamoDB Payload<pre><code>item = {\n    'id': meeting_id, \n    'teacher_name': payload.get('teacher_name'),\n    'student_name': payload.get('student_name'),\n    'meet_time': meet_time_str, \n    'jitsi_url': jitsi_url,\n    'status': 'SCHEDULED', \n    'created_at': datetime.now(timezone.utc).isoformat()\n}\n</code></pre> <ul> <li>Note that this is the payload from the frontend, plus a few more params created in this function.</li> </ul>"},{"location":"Phase%203/schedule-meeting-lambda/#lambda-response","title":"Lambda Response","text":"Lambda Response<pre><code>return {\n    'statusCode': 200,\n    'headers': {\n        'Content-Type': 'application/json',\n        'Access-Control-Allow-Origin': '*',\n        'Access-Control-Allow-Methods': 'OPTIONS,POST'\n    },\n    'body': json.dumps({\n        'message': 'Session scheduled!', \n        'id': meeting_id,\n        'jitsi_url': jitsi_url,\n        'teacher_link': teacher_link, \n        'student_link': student_link \n    })\n}\n</code></pre> <ul> <li>Once this function is successful, it's not just gonna send back \"success\" to the frontend. It's going to create the links for both the teacher and student to join their provisioned EC2 instances.</li> <li>The frontend receives that <code>statusCode</code> 200 and will redirect to a modal providing the meeting links.</li> </ul> <p>You'll notice that the links follow a specific format:</p> Link Format<pre><code>teacher_link = f\"{frontend_base_url}?id={meeting_id}&amp;role=teacher\"\n</code></pre> <p>Testing - Lambda Function</p> <p>To test this Lambda, there is a test panel in the Lambda service. You can send a mock payload, which will actually write to your DB.</p>"},{"location":"Phase%203/schedule-meeting-lambda/#iam-roles","title":"IAM Roles","text":"<p>You'll need a special IAM role for this Lambda. We previously set up IAM roles in Phase 2, so I'll speed through this.</p>"},{"location":"Phase%203/schedule-meeting-lambda/#role-1-awslambdabasicexecutionrole","title":"Role 1: AWSLambdaBasicExecutionRole","text":"<ul> <li>Pre-defined by AWS</li> </ul>"},{"location":"Phase%203/schedule-meeting-lambda/#role-2-schedulerlogic","title":"Role 2: SchedulerLogic","text":"SchedulerLogic Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DatabaseWrite\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"dynamodb:PutItem\",\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/elephant-meetings\"\n        },\n        {\n            \"Sid\": \"CreateTimer\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"scheduler:CreateSchedule\",\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"HandOverKeyCard\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\",\n            \"Resource\": \"arn:aws:iam::034489661489:role/ElephantSchedulerRole\"\n        }\n    ]\n}\n</code></pre>"},{"location":"Phase%203/schedule-meeting-lambda/#role-3-write-meetings","title":"Role 3: Write Meetings","text":"Write Meetings Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"dynamodb:PutItem\",\n            \"Resource\": \"arn:aws:dynamodb:*:034489661489:table/*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"Phase%203/schedule-meeting-lambda/#environment-variables","title":"Environment Variables","text":"<p>Also, you're going to need a few environment variables:</p> Key <code>LAUNCHER_ARN</code> <code>SAFETY_NET_ARN</code> <code>SCHEDULER_ROLE_ARN</code> <p>Note - Environment Variables</p> <p>You can skip or leave the top two blank since we haven't written them yet. We're going to define a specific IAM role for the EventBridge scheduler within the schedule-meeting Lambda.</p> <p>Warning - IAM Role Names</p> <p>WARNING: they do NOT have the same roles, despite having similar names!</p>"},{"location":"Phase%203/schedule-meeting-lambda/#eventbridge-scheduler-role","title":"EventBridge Scheduler Role","text":"<p>Create a new IAM role, and apply the pre-defined AWS role called <code>LambdaInvoke</code>. If you don't see that, this is the policy:</p> LambdaInvoke Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"lambda:InvokeFunction\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>Copy the ARN in the top right of the IAM role and add it as an environment variable. This is what gives permissions to the scheduler to call another Lambda.</p> <p>And that's the gist of the first Lambda, <code>schedule-meeting</code>. Now let's set up API Gateway so that we can hit it.</p> <p>Next: API Gateway and Testing \u2192</p>"},{"location":"Phase%204/","title":"Phase 4: Spin Up, DNS and Frontend Redirect","text":""},{"location":"Phase%204/#overview","title":"Overview","text":"<p>At this point, we have our first Lambda where we handle meeting scheduling. Now we're gonna go ballistic on the Lambdas. Things are gonna go pretty quick, but don't worry, wherever we get a bit funky I'll slow down.</p> <p>Warning - Domain Required</p> <p>BEFORE WE START: you need a domain. If you haven't had any issues with SSL certificates and connectivity yet, you are about to. The moment you start using the DCV SDK, using WebSocket or anything TCP / UDP, you will get screwed.</p> <p>You don't need to use Cloudflare like me, but these docs will discuss Cloudflare. The main steps are the same for all DNS providers: you will need to be able to install an origin certificate on the EC2, enable HTTPS / SSL certificates, including locally, and be able to create an A certificate for a specific domain. You can use Route 53 and other DNS management services in AWS if you are more comfortable.</p> <p>This will probably be the most complex section of this project.</p>"},{"location":"Phase%204/#what-were-building","title":"What We're Building","text":"<p>Here's what we're going to do:</p> <ul> <li>After <code>schedule-meeting</code>, EventBridge scheduler will run <code>launch-ec2</code>, which will spin up two EC2 instances from an AMI.</li> <li><code>ElephantDNSWatchdog</code> will wait for the state of the EC2 to change from \"initializing\" to \"running\". Once it does, it will use the Cloudflare API to create a new A certificate on your domain. It will also write the EC2 URL to the DynamoDB.</li> <li>Then <code>dcv-url</code> will fetch the URL and redirect the user in the frontend.</li> </ul> <p>Later on, when we configure the spin down, we're going to have a 15 minute safety net where we kill any EC2 instances not being used. Then our actual meeting termination will be handled by the Stream feature with DynamoDB, that will send any modifications as an object to the termination Lambda.</p> <p></p> <p>Note - Lambda Architecture</p> <p>So yeah, like 6 Lambdas in this section? And honestly, the functions of some of them could be decoupled into more. The moment you need data to move across an architecture, or have one service to talk to another, that's a new Lambda. They're really like spaghetti strands.</p>"},{"location":"Phase%204/#what-youll-learn","title":"What You'll Learn","text":"<p>This phase covers:</p> <ol> <li>Certificates in Cloudflare - Set up SSL certificates for secure DCV connections</li> <li>Getting a Token from Cloudflare - Obtain API token for DNS management</li> <li>Launch EC2 - Create Lambda to programmatically spin up EC2 instances</li> <li>ElephantDNSWatchdog - Monitor EC2 state changes and manage DNS records</li> <li>Frontend Implementation - Integrate meeting links and redirects</li> <li>Get DCV URL - Lambda to fetch and redirect users to their EC2 instances</li> <li>Spin Down - Configure termination Lambdas for safe EC2 instance shutdown</li> </ol> <p>Let's get started with Certificates in Cloudflare \u2192</p>"},{"location":"Phase%204/certificates-cloudflare/","title":"Certificates in Cloudflare","text":"<p>The first thing you need to do is get your SSL and Origin certificates in order on your EC2. We'll handle this in three parts.</p>"},{"location":"Phase%204/certificates-cloudflare/#part-1-generate-certificate-cloudflare","title":"Part 1: Generate Certificate (Cloudflare)","text":"<ol> <li>Log in to Cloudflare and navigate to SSL/TLS &gt; Origin Server.</li> <li>Click Create Certificate.</li> <li>Enter your subdomain (e.g., <code>dcv.your-domain.com</code>) in Hostnames and click Create.</li> <li>Copy the contents of Origin Certificate and Private Key separately. Do not close the window yet.</li> </ol>"},{"location":"Phase%204/certificates-cloudflare/#part-2-install-on-ec2","title":"Part 2: Install on EC2","text":"<ol> <li>RDP into your EC2 instance as Administrator.</li> <li>Navigate to: <code>C:\\Windows\\System32\\config\\systemprofile\\AppData\\Local\\NICE\\dcv\\</code></li> <li>Create two files here using Notepad (save as <code>All Files</code>):<ul> <li><code>dcv.pem</code>: Paste the Origin Certificate content.</li> <li><code>dcv.key</code>: Paste the Private Key content.</li> </ul> </li> <li>Restart the DCV Service:<ul> <li>Run <code>services.msc</code>.</li> <li>Right-click DCV Server &gt; Restart.</li> </ul> </li> </ol>"},{"location":"Phase%204/certificates-cloudflare/#part-3-finalize-cloudflare","title":"Part 3: Finalize (Cloudflare)","text":"<ol> <li>Go to SSL/TLS &gt; Overview and set mode to Full (Strict).</li> <li>Go to DNS &gt; Records.</li> <li>Ensure the record for your DCV subdomain is set to Proxied (orange cloud).</li> </ol> <p>Testing - Secure Connection</p> <p>You can now connect securely via <code>https://dcv.your-domain.com:8443</code>.</p>"},{"location":"Phase%204/certificates-cloudflare/#creating-an-ami","title":"Creating an AMI","text":"<p>Now, whatever EC2 you have, it might not be the finished product for this project, but for now you're going to want to create an AMI from it, which is super easy. Go to the EC2 dashboard, select your EC2, and go create an image.</p> <p></p> <p>Give it a name, and ensure that the storage size is the same as what you use in your EC2.</p> <p>Note - AMI Creation State</p> <p>Note: you can only create an AMI while the EC2 is running or stopped; different resources might claim that it's better when stopped or running, in my experience it really hasn't mattered.</p> <p>Now that you have that, configure the launch-ec2 Lambda.</p> <p>Next: Launch EC2 \u2192</p>"},{"location":"Phase%204/cloudflare-token/","title":"Getting a Token from Cloudflare","text":"<p>This section covers obtaining a Cloudflare API token that will be used by the <code>ElephantDNSWatchdog</code> Lambda to manage DNS records programmatically.</p>"},{"location":"Phase%204/cloudflare-token/#part-1-get-the-token-from-cloudflare","title":"Part 1: Get the Token from Cloudflare","text":"<ol> <li>Log in to the Cloudflare Dashboard.</li> <li>Click the User Icon (top right) &gt; My Profile.</li> <li>Go to API Tokens (left sidebar) &gt; Create Token.</li> <li>Use the Edit Zone DNS template.</li> <li>Under Zone Resources, select:<ul> <li><code>Include</code> &gt; <code>Specific zone</code> &gt; <code>your-domain.com</code> (e.g., christardy.com)</li> </ul> </li> <li>Click Continue to summary &gt; Create Token.</li> <li>Copy the token immediately. (You won't be able to see it again).</li> </ol>"},{"location":"Phase%204/cloudflare-token/#part-2-store-it-in-aws-secrets-manager","title":"Part 2: Store it in AWS Secrets Manager","text":"<ol> <li>Log in to the AWS Console and search for Secrets Manager.</li> <li>Click Store a new secret.</li> <li>Select \"Other type of secret\".</li> <li>In the Key/value pairs section:<ul> <li>Key: <code>CLOUDFLARE_API_TOKEN</code></li> <li>Value: (Paste your Cloudflare token here)</li> </ul> </li> <li>Click Next.</li> <li>Secret name: <code>ElephantCloudflareToken</code> (or a name your Lambda expects).</li> <li>Click Next, keep defaults, and click Store.</li> </ol> <p>Note - Token Security</p> <p>Your <code>ElephantDNSWatchdog</code> Lambda can now securely retrieve this token to update DNS records.</p> <p>Warning - Token Storage</p> <p>Make sure to store the token securely. You won't be able to retrieve it again from Cloudflare if you lose it.</p> <p>This token will be used in the ElephantDNSWatchdog \u2192 section.</p>"},{"location":"Phase%204/dns-watchdog/","title":"ElephantDNSWatchdog","text":"<p>This is the function that is going to wait for the state of the launched EC2's to change to \"Running\", and it's going to do this via a service called EventBridge (CloudWatch Events). We are also going to need to get the API token from Cloudflare and stick it in AWS Secrets Manager.</p> <p>Note - Cloudflare Token Reference</p> <p>Refer to the Getting a Token from Cloudflare section for instructions on obtaining and storing the Cloudflare API token.</p>"},{"location":"Phase%204/dns-watchdog/#eventbridge-cloudwatch-events","title":"EventBridge (CloudWatch Events)","text":"<p>Go to EventBridge. Then go to Rules on the left tab.</p> <p></p>"},{"location":"Phase%204/dns-watchdog/#step-1-create-the-rule","title":"Step 1: Create the Rule","text":"<ol> <li>Click Create rule.</li> <li>Name: <code>ElephantEC2StateWatch</code> (or similar).</li> <li>Rule type: Select Rule with an event pattern.</li> <li>Click Next.</li> </ol>"},{"location":"Phase%204/dns-watchdog/#step-2-build-the-event-pattern","title":"Step 2: Build the Event Pattern","text":"<p>This is the critical part. You want to filter for EC2 state changes but only for instances tagged for Project Elephant.</p> <ol> <li>Scroll down to Event pattern.</li> <li>Event source: <code>AWS services</code>.</li> <li>AWS service: <code>EC2</code>.</li> <li>Event type: <code>EC2 Instance State-change Notification</code>.</li> <li>Event pattern content: Click Edit pattern (JSON) and paste this exactly. This filters for \"running\" (to create DNS) and \"shutting-down\" (to delete DNS), but only if the instance has the tag <code>Project: Elephant</code>.</li> </ol> EventBridge Event Pattern<pre><code>{\n  \"source\": [\"aws.ec2\"],\n  \"detail-type\": [\"EC2 Instance State-change Notification\"],\n  \"detail\": {\n    \"state\": [\"running\", \"shutting-down\"],\n    \"tags\": {\n      \"Project\": [\"Elephant\"]\n    }\n  }\n}\n</code></pre> <ol> <li>Click Next.</li> </ol>"},{"location":"Phase%204/dns-watchdog/#step-3-select-the-target","title":"Step 3: Select the Target","text":"<ol> <li>Target types: Select AWS service.</li> <li>Select a target: Choose Lambda function.</li> <li>Function: Select <code>ElephantDNSWatchdog</code>.</li> <li>Click Next, skip the tags screen, and click Create rule.</li> </ol> <p>Awesome - EventBridge Automation</p> <ul> <li>Startup: When your Launcher spins up an EC2 and tags it <code>Project: Elephant</code>, the instance enters the <code>running</code> state. EventBridge sees this and fires the Lambda. The Lambda sees \"running\", gets the IP, and creates the DNS record.</li> <li>Shutdown: When your script runs <code>shutdown</code> (or you manually terminate), the instance enters <code>shutting-down</code>. EventBridge fires the Lambda. The Lambda sees \"shutting-down\", looks up the ID, and deletes the DNS record.</li> </ul> <p>Pretty cool, right?</p> <p></p>"},{"location":"Phase%204/dns-watchdog/#environment-variables","title":"Environment Variables","text":"Key Value DOMAIN_ROOT <code>your-domain</code> SECRET_ARN <code>prod/elephant/cloudflare</code> ZONE_ID <code>zone id from Cloudflare</code> <p>Note - Environment Variables</p> <p>You should be getting pretty much all of these from Cloudflare or your DNS provider.</p>"},{"location":"Phase%204/dns-watchdog/#code-breakdown","title":"Code Breakdown","text":"Initial Setup<pre><code># Initialize Clients\nec2 = boto3.client('ec2')\nsecrets = boto3.client('secretsmanager')\nhttp = urllib3.PoolManager()\ndynamodb = boto3.resource('dynamodb', region_name='us-west-2')\n\n# --- CONFIGURATION ---\nSECRET_ARN = os.environ['SECRET_ARN'] \nZONE_ID = os.environ['ZONE_ID'] \nDOMAIN_ROOT = os.environ['DOMAIN_ROOT']\nTABLE_NAME = 'elephant-meetings'\n</code></pre> <ul> <li>Pretty standard at this point. Just variable setup.</li> </ul> Fetching the Secret<pre><code>resp = secrets.get_secret_value(SecretId=SECRET_ARN)\nreturn json.loads(resp['SecretString'])['CLOUDFLARE_TOKEN']\n</code></pre> <ul> <li>Fetch the secret from AWS Secrets Manager</li> </ul> Creating DNS Records<pre><code>if action == 'CREATE':\n    if not ip:\n        print(\"Error: CREATE action requires an IP address.\")\n        return False\n\n    print(f\"Pointing {record_name} -&gt; {ip}\")\n    payload = {\n        \"type\": \"A\", \n        \"name\": record_name, \n        \"content\": ip, \n        \"ttl\": 120,      \n        \"proxied\": True \n    }\n</code></pre> <p>Note - A Record Explanation</p> <p>Hey look! A payload. Yeah, we're sending to the Cloudflare API endpoint a payload with the certificate that we want to create. In this situation, we want to create an A certificate for the instance_id whose state changed. It'll be like <code>i-abc123</code>.</p> <p>An A certificate is a specific kind of certificate in DNS. It basically just says \"all traffic that hits this subdomain, just point it here.\" So if I create an A certificate for <code>edmontonoilers.christardy.com</code>, I can send anyone who hits that URL to <code>https://www.nhl.com/canucks/</code>. Sorry, Connor.</p> Database Update After DNS Creation<pre><code>if action == 'CREATE_SUCCESS':\n    # Status update after DNS record is created\n    status = 'DNS_WRITTEN'\n\n    # Construct the final DCV URL\n    dcv_url = f\"https://{instance_id}.{DOMAIN_ROOT}:8443\"\n\n    # Dynamically set the key based on the role\n    url_field = f\"{role}_ec2_link\" # teacher_ec2_link or student_ec2_link\n\n    print(f\"DB WRITE: Setting status to {status} and saving URL {dcv_url}\")\n\n    try:\n        table.update_item(\n            Key={'id': meeting_id},\n            UpdateExpression=f\"SET #s = :new_status, {url_field} = :url\",\n            ExpressionAttributeNames={'#s': 'status'},\n            ExpressionAttributeValues={\n                ':new_status': status,\n                ':url': dcv_url\n            },\n            ReturnValues=\"UPDATED_NEW\"\n        )\n    except Exception as e:\n</code></pre> <ul> <li>This updates our DynamoDB after the A certificates have been created. At this point, we have a pretty nifty piece of data: we have a deterministic URL for how we're going to access the EC2 instance in the frontend. The format is basically:</li> </ul> DCV URL Format<pre><code>https://ec2-id.domain.com:8443\n</code></pre> <p>Note - DNS Status</p> <p>You'll see that our next Lambda will check to see if this URL is present. \"DNS_WRITTEN\" is a status that you can use for testing.</p> DNS Deletion on Shutdown<pre><code>elif state == 'shutting-down':\n    if update_dns('DELETE', record_name):\n        update_db_on_dns_event(meeting_id, role, 'DELETE_SUCCESS')\n</code></pre> <ul> <li>This is the last piece that deletes the A certificate. When we terminate an EC2, it first shuts down, which prompts this watchdog function to delete the A certificates in Cloudflare.</li> </ul> <p>Awesome - Stateless Lambda Intelligence</p> <p>As you can see, the Watchdog is a particularly intelligent Lambda, because while Lambdas are serverless and stateless, this Lambda is always ready to receive events.</p>"},{"location":"Phase%204/dns-watchdog/#iam-role","title":"IAM Role","text":"<ul> <li>AWSLambdaBasicExecutionRole, pre-built by Amazon</li> <li>SecretsManager read, which allows us to get the API token.</li> </ul> SecretsManager Read Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"ec2:DescribeInstances\",\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"secretsmanager:GetSecretValue\",\n            \"Resource\": \"arn:aws:secretsmanager:us-west-2:*:secret:prod/elephant/cloudflare-*\"\n        }\n    ]\n}\n</code></pre> <ul> <li>WatchdogUpdateItem, allows read/write to DynamoDB</li> </ul> WatchdogUpdateItem Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:UpdateItem\",\n                \"dynamodb:GetItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:us-west-2:034489661489:table/elephant-meetings\"\n        }\n    ]\n}\n</code></pre> <p>Next: Frontend Implementation \u2192</p>"},{"location":"Phase%204/frontend-implementation/","title":"Frontend Implementation","text":"<p>Alright, so we're gonna test this whole process. Let's go ahead and schedule a meeting.</p> <p></p> <p>Note - Placeholder Names</p> <p>(Boonki Goongo : all my placeholder students are from this video)</p> <p>If the frontend is working correctly (if not, check the developer console in your browser), you will schedule the meeting, and get to a page like this:</p> <p></p> <p>These are the meeting links for both the student and the teacher. The id for the DynamoDB row and their role are passed in as parameters in the header. This will enable the redirect via the get-dcv-url Lambda. Copy one, and it will take you here:</p> <p></p> <p>Once media permissions are enabled, the \"Launch My Meeting\" button becomes available. Which is going to trigger a new API Gateway Endpoint and Lambda that we will discuss now.</p> <p>Next: Get DCV URL \u2192</p>"},{"location":"Phase%204/get-dcv-url/","title":"Get DCV URL","text":"<p>The first thing is that we need to define a new API Gateway URL. This one is going to be a bit more complicated than the first one, just follow these steps.</p>"},{"location":"Phase%204/get-dcv-url/#api-gateway-setup","title":"API Gateway Setup","text":""},{"location":"Phase%204/get-dcv-url/#1-create-the-api","title":"1. Create the API","text":"<ol> <li>Navigate to API Gateway in the AWS Console.</li> <li>Click Create API &gt; REST API &gt; Build.</li> <li>Settings:<ul> <li>Protocol: <code>REST</code></li> <li>Create new API: <code>New API</code></li> <li>API name: <code>ElephantJoinAPI</code></li> <li>Endpoint Type: <code>Regional</code></li> </ul> </li> <li>Click Create API.</li> </ol>"},{"location":"Phase%204/get-dcv-url/#2-build-the-resource-structure","title":"2. Build the Resource Structure","text":"<p>You need to create a nested path: <code>/join/{meetingId}/{role}</code>.</p> <ol> <li>Create <code>/join</code>:<ul> <li>Select the root (<code>/</code>) &gt; Actions &gt; Create Resource.</li> <li>Resource Name: <code>join</code></li> <li>Click Create Resource.</li> </ul> </li> <li>Create <code>/{meetingId}</code>:<ul> <li>Select <code>/join</code> &gt; Actions &gt; Create Resource.</li> <li>Resource Name: <code>meetingId</code></li> <li>Resource Path: <code>{meetingId}</code> (Ensure curly braces are used).</li> <li>Click Create Resource.</li> </ul> </li> <li>Create <code>/{role}</code>:<ul> <li>Select <code>/{meetingId}</code> &gt; Actions &gt; Create Resource.</li> <li>Resource Name: <code>role</code></li> <li>Resource Path: <code>{role}</code></li> <li>Click Create Resource.</li> </ul> </li> </ol>"},{"location":"Phase%204/get-dcv-url/#3-configure-the-get-method","title":"3. Configure the GET Method","text":"<ol> <li>Select the <code>/{role}</code> resource.</li> <li>Click Actions &gt; Create Method.</li> <li>Select GET from the dropdown and click the checkmark.</li> <li>Setup Integration:<ul> <li>Integration type: <code>Lambda Function</code></li> <li>Use Lambda Proxy integration: <code>Check</code> (Recommended for easy access to path parameters).</li> <li>Lambda Function: Enter <code>GetDcvUrl</code> (or your specific redirection Lambda name).</li> <li>Click Save.</li> </ul> </li> </ol>"},{"location":"Phase%204/get-dcv-url/#4-deploy","title":"4. Deploy","text":"<ol> <li>Click Actions &gt; Deploy API.</li> <li>Deployment stage: <code>[New Stage]</code> (e.g., <code>prod</code>).</li> <li>Click Deploy.</li> </ol> <p>Your API should look like this:</p> <p></p> <p>Awesome - Link as Storage</p> <p>This allows us to pass in headers into the API and still use the GET method. This is ideal because then there's no middle application that needs to store the payload for when the users want to enter their meeting. Think about it, the meeting could be a week away. This link is effectively the storage! Pretty sick.</p> <p>Now, let's get to the Lambda.</p>"},{"location":"Phase%204/get-dcv-url/#environment-variables","title":"Environment Variables","text":"<p>None :-)</p>"},{"location":"Phase%204/get-dcv-url/#code-breakdown","title":"Code Breakdown","text":"Path Parameter Parsing<pre><code>path_params = event.get('pathParameters', {})\nmeeting_id = path_params.get('meetingId')\nuser_role = path_params.get('role') \n\nif not meeting_id or user_role not in ['teacher', 'student']:\n    return {\n        'statusCode': 400, \n        'body': json.dumps({'message': 'Invalid meeting ID or role.'}), \n        'headers': {'Access-Control-Allow-Origin': '*'}\n    }\n</code></pre> <ul> <li>The frontend is going to send the primary key from DynamoDB and the role via the header, and the Lambda is going to parse those.</li> </ul> Fetching the DCV URL<pre><code>link_field = f\"{user_role}_ec2_link\" \nfinal_dcv_url = item.get(link_field)\n</code></pre> <ul> <li>This either pulls from DynamoDB the column of <code>student_ec2_link</code> or <code>teacher_ec2_link</code>.</li> </ul> Status Check Guard<pre><code>if not final_dcv_url or current_status not in ['HEALTHY', 'IN_PROGRESS']:\n    print(f\"Waiting for ready state. Link: {bool(final_dcv_url)}, Status: {current_status}\")\n    return {\n        'statusCode': 202, # Signal to Frontend: \"Accepted, keep polling.\"\n        'headers': {'Access-Control-Allow-Origin': '*'},\n        'body': json.dumps({\n            'message': 'Instance is warming up. Please wait.', \n            'status': current_status \n        })\n    }\n</code></pre> <p>Note - Health Check Status</p> <p>We need a confirmation that the EC2 is ready for the user, which is what this guard is for. \"Healthy\" comes from the health check which is covered in another Phase.</p> Status Update<pre><code>try:\n    table.update_item(\n        Key={'id': meeting_id},\n        UpdateExpression=\"SET #s = :new_status\",\n        ExpressionAttributeNames={'#s': 'status'},\n        ExpressionAttributeValues={':new_status': 'IN_PROGRESS'}\n    )\n</code></pre> <ul> <li>We write to DynamoDB the status of <code>IN_PROGRESS</code>.</li> <li><code>IN_PROGRESS</code> is a signal that the other user is already in the session. We use it as a guard for termination, which we'll get to after this.</li> </ul>"},{"location":"Phase%204/get-dcv-url/#iam-role","title":"IAM Role","text":"<ul> <li>AWSLambdaBasicExecutionRole, pre-built by AWS</li> <li>DynamoRead, custom inline</li> </ul> DynamoRead Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"dynamodb:GetItem\",\n            \"Resource\": \"arn:aws:dynamodb:us-west-2:034489661489:table/elephant-meetings\"\n        }\n    ]\n}\n</code></pre> <p>Awesome - Complete Flow</p> <p>At this point, you should be able to schedule a meeting, and watch the EC2 instances spin up, and then join them via a redirect URL. Like, just take a pause. We just provisioned virtual machines and fully redirected users into them, and it only took like 4 python scripts and a few extra services. The internet is lit.</p> <p>Congratulations! You've completed Phase 4. You should now have a complete flow from scheduling a meeting to launching EC2 instances and redirecting users to their dedicated virtual machines.</p>"},{"location":"Phase%204/launch-ec2/","title":"Launch EC2","text":"<p>Since we have a lot of Lambdas in this section, it's going to follow a mainly fixed format: environment variables, code breakdown, IAM roles.</p>"},{"location":"Phase%204/launch-ec2/#environment-variables","title":"Environment Variables","text":"Key Value AMI_ID <code>ami-0e7fd79d17e852d18</code> IAM_PROFILE <code>EC2-S3-ReadOnly-Role</code> INSTANCE_TYPE <code>t3.large</code> KEY_NAME <code>shaundano-elephant</code> SECURITY_GROUP_IDS <code>sg-0ca7703192305868d,sg-0c0013f4412c040be</code> <p>Note - Environment Variables</p> <p>Note that the security groups are the ones that you configured for your EC2 back in Phase 1, and the key name refers to your public and private keys when creating your EC2 as well. This basically allows you to spin up an EC2 instance programmatically.</p>"},{"location":"Phase%204/launch-ec2/#code-breakdown","title":"Code Breakdown","text":"Initial Setup<pre><code># Force us-west-2 to ensure we hit the right Table and EC2 region\nec2 = boto3.client('ec2', region_name='us-west-2')\ndynamodb = boto3.resource('dynamodb', region_name='us-west-2')\n\nTABLE_NAME = 'elephant-meetings'\ntable = dynamodb.Table(TABLE_NAME)\n\nami_id = os.environ['AMI_ID']\ninstance_type = os.environ['INSTANCE_TYPE']\nkey_name = os.environ['KEY_NAME']\niam_profile = os.environ['IAM_PROFILE']\nsg_ids = os.environ['SECURITY_GROUP_IDS'].split(',')\n</code></pre> <ul> <li>Defining some local and environment variables. Everything with <code>os.</code> uses the <code>os</code> library which allows us to use environment variables configured in the Lambda GUI.</li> </ul> Lambda Handler<pre><code>def lambda_handler(event, context):\n</code></pre> <p>Note - EventBridge Event Object</p> <p>Remember how we used EventBridge scheduler in schedule-meeting? This is coming back around. That event object in the lambda_handler IS the event we scheduled. This event is the payload of everything this Lambda needs.</p> Availability Zone Selection<pre><code>vpcs = ec2.describe_vpcs(Filters=[{'Name': 'isDefault', 'Values': ['true']}])\ndefault_vpc = vpcs['Vpcs'][0]['VpcId']\nsubnets = ec2.describe_subnets(Filters=[{'Name': 'vpc-id', 'Values': [default_vpc]})\n\nvalid_subnet_id = None\nsupported_azs = ['us-west-2a', 'us-west-2b', 'us-west-2c']\n\nfor subnet in subnets['Subnets']:\n    if subnet['AvailabilityZone'] in supported_azs:\n        valid_subnet_id = subnet['SubnetId']\n        break\n</code></pre> <p>Note - Availability Zone Selection</p> <p>This entire chunk is AWS looking for an availability zone to spin up the EC2 instance. This is more relevant for GPU EC2 instances. They are in limited supply, and sometimes your default zone and region won't have any GPUs available, especially from 9-5. Considering that we are now using non-GPU instances, this is less necessary, but it's still good for redundancy.</p> Database State Check<pre><code>try:\n    print(f\"Checking existing state for: {meeting_id}\")\n    current_state = table.get_item(Key={'id': meeting_id})\n    current_item = current_state.get('Item', {})\nexcept Exception as e:\n    # If we can't read the DB, fail safely rather than launching blind\n    print(f\"CRITICAL: Could not read DB state. Aborting to prevent duplicates. Error: {e}\")\n    raise e\n</code></pre> <p>Warning - Cost Protection</p> <p>The payload gave us the primary key for the DynamoDB, and this is a failsafe in case the id doesn't link to a row in DynamoDB. EC2 costs money, so we don't want to spin up instances for a broken process.</p> Instance Launch Loop<pre><code>roles = ['teacher', 'student']\nlaunched_ids = {}\n</code></pre> <ul> <li>We're going to effectively run the launch function twice for each role.</li> </ul> EC2 Launch Configuration<pre><code>response = ec2.run_instances(\n    ImageId=ami_id,\n    InstanceType=instance_type,\n    KeyName=key_name,\n    SecurityGroupIds=sg_ids,\n    SubnetId=valid_subnet_id,\n    IamInstanceProfile={'Name': iam_profile},\n    MinCount=1,\n    MaxCount=1,\n    InstanceInitiatedShutdownBehavior='terminate',\n    # OPTIONAL EXTRA SAFETY: AWS ClientToken\n    # Even with the DB check, this guarantees AWS itself won't double-provision \n    # if the script runs twice at the exact same millisecond.\n    ClientToken=f\"{meeting_id}-{role}\", \n    BlockDeviceMappings=[{\n        'DeviceName': '/dev/xvda',\n        'Ebs': {'VolumeSize': 60, 'VolumeType': 'gp2', 'DeleteOnTermination': True}\n    }],\n    TagSpecifications=[{\n        'ResourceType': 'instance',\n        'Tags': [\n            {'Key': 'Name', 'Value': instance_name},\n            {'Key': 'MeetingID', 'Value': meeting_id},\n            {'Key': 'Role', 'Value': role},\n            {'Key': 'Project', 'Value': 'Elephant'}\n        ]\n    }]\n)\n</code></pre> <p>Note - Launch Parameters</p> <p>Ok, this chunk is a bit hefty. We are launching using a bunch of parameters from the environment variables. It will be SUPER easy to switch to a new AMI once you finish working on your EC2, and the same if you decide to change your instance type.</p> <p>Awesome - ClientToken Protection</p> <p>I found in the past that there would be some sort of race condition or retry logic with this launch function such that it would trigger several times, which is not ideal. The ClientToken is a built-in AWS service that ensures that this whole function will only run once.</p> DynamoDB Update<pre><code>db_resp = table.update_item(\n    Key={'id': meeting_id},\n    UpdateExpression=f\"set {column_name} = :i, #s = :s\",\n    ExpressionAttributeNames={'#s': 'status'},\n    ExpressionAttributeValues={\n        ':i': inst_id,\n        ':s': 'LAUNCHED'\n    },\n    ReturnValues=\"UPDATED_NEW\"\n)\n</code></pre> <ul> <li>This updates the DynamoDB for the given id once the EC2 has been successfully launched. It puts the two instance id's in their necessary place: <code>teacher_ec2_id</code> and <code>student_ec2_id</code></li> </ul>"},{"location":"Phase%204/launch-ec2/#iam-role","title":"IAM Role","text":"<p>Not for the EC2s, which should use your existing EC2 role.</p> <p>Here's my EC2 Launcher Role:</p> <ul> <li>AWSLambdaBasicExecutionRole, pre-built by AWS</li> <li>EC2 Launch custom inline policy</li> </ul> EC2 Launch Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:RunInstances\",\n                \"ec2:CreateTags\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeInstances\",\n                \"ec2:TerminateInstances\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\",\n            \"Resource\": \"arn:aws:iam::034489661489:role/EC2-S3-ReadOnly-Role\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:GetItem\",\n                \"dynamodb:UpdateItem\",\n                \"dynamodb:PutItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/elephant-meetings\"\n        }\n    ]\n}\n</code></pre> <p>Next: ElephantDNSWatchdog \u2192</p>"},{"location":"Phase%204/spin-down/","title":"Spin Down","text":"<p>We just have two more Lambdas at the end that handle termination. One is called <code>meeting-safety-net</code>. The next one is called <code>terminator</code>.</p> <p>Note - Logic Flow</p> <p>(logic chart right here via Figma)</p>"},{"location":"Phase%204/spin-down/#turning-on-streaming-in-dynamodb","title":"Turning on Streaming in DynamoDB","text":"<p>Go to <code>exports and streams</code> in your DynamoDB table. Scroll down to stream details, and turn this on. We're gonna come back to it. Basically, DynamoDB can stream event objects to specific Lambda functions, if you set up a trigger. You know how the <code>lambda_handler</code> function typically takes an event as a parameter? Well, the event can come from DynamoDB whenever it changes, like an interrupt. The Lambda doesn't even need to poll the table.</p> <p></p>"},{"location":"Phase%204/spin-down/#safety-net","title":"Safety Net","text":"<p>This function was actually part of <code>schedule-meeting</code>. Remember in the configuration of the environment variables?</p> <p></p> <p>The safety net points to the ARN of this Lambda. Here's the chunk of code that schedules it within the <code>schedule-meeting</code> Lambda:</p> Scheduling the Safety Net<pre><code>safety_dt = meet_dt + timedelta(minutes=15)\nsafety_iso = safety_dt.strftime('%Y-%m-%dT%H:%M:%S')\n\nprint(f\"Scheduling Safety Net for: {safety_iso}\")\n\nscheduler.create_schedule(\n    Name=f\"safety-{meeting_id}\",\n    ScheduleExpression=f\"at({safety_iso})\",\n    Target={\n        'Arn': safety_net_arn,\n        'RoleArn': scheduler_role,\n        # Note: Safety Net Lambda expects 'meetingId' (CamelCase)\n        'Input': json.dumps({'meetingId': meeting_id}) \n    },\n    FlexibleTimeWindow={'Mode': 'OFF'},\n    ActionAfterCompletion='DELETE'\n)\n</code></pre> <p>This will automatically trigger ONCE 15 minutes after the meeting time.</p>"},{"location":"Phase%204/spin-down/#code-breakdown","title":"Code Breakdown","text":"Safety Net Logic<pre><code># Rule A: No Show\nif not has_teacher and not has_student:\n    # EXCEPTION: If status is IN_PROGRESS, we assume they are talking but haven't uploaded yet.\n    if status == 'IN_PROGRESS':\n        print(f\"Meeting {meeting_id} is IN_PROGRESS with no data. Assuming long meeting. KEEPING ALIVE.\")\n        return\n    else:\n        should_kill = True\n        reason = \"NO_SHOW_TIMEOUT\"\n\n# Rule B: Partial (One side uploaded, other never showed up or crashed hard)\nelif has_teacher != has_student:\n    should_kill = True\n    reason = \"PARTIAL_NO_SHOW_TIMEOUT\"\n\n# Rule C: Early Success\n# If we are here at T+15m and both have data, it means the Stream Terminator didn't kill it\n# (likely because it was &lt; 15 mins duration).\nelif has_teacher and has_student:\n    should_kill = True\n    reason = \"15_MIN_SUCCESS\"\n</code></pre> <p>Note - Safety Net Termination Rules</p> <p>This is basically the entire safety net logic statement. It will kill the EC2 instances in the following situations at the 15-minute mark:</p> <ul> <li>No Show: If both teacher and student data are empty AND the meeting doesn't say <code>IN_PROGRESS</code>, then we assume that nobody showed up</li> <li>Partial No Show: If only one person has data written, someone showed up and waited for the other person, but they never showed up</li> <li>Early Success: Both student and teacher have data written, suggesting a very short meeting.</li> </ul> Terminating Instances<pre><code>if should_kill:\n    print(f\"SAFETY NET: Terminating {meeting_id}. Reason: {reason}\")\n\n    # Collect all possible Instance IDs to be safe\n    targets = []\n    if 'teacher_ec2_id' in item: targets.append(item['teacher_ec2_id'])\n    if 'student_ec2_id' in item: targets.append(item['student_ec2_id'])\n\n    targets = list(set([t for t in targets]))\n\n    if targets:\n        try:\n            ec2.terminate_instances(InstanceIds=targets)\n            print(f\"Terminated EC2s: {targets}\")\n</code></pre> <p>If <code>should_kill</code> is set to true by any of the logic statements, then we proceed with terminating all instances using their IDs.</p> <p>Warning - Safety Net Design</p> <p>Note that I came up with this safety net logic as a proposed solution to some user situations. In a perfect situation, if compute and cost wasn't constrained, maybe a solution would be to ask the teacher, \"how long will your meeting be\"? And give them a number of hours per month. That way, you can just have it on a timer. It's up to you.</p>"},{"location":"Phase%204/spin-down/#iam-role","title":"IAM Role","text":"<ul> <li>AWSLambdaBasicExecutionRole, pre-built by Amazon</li> <li>MeetingSafetyNetPolicy, custom inline</li> </ul> MeetingSafetyNetPolicy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowDynamoDBAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:GetItem\",\n                \"dynamodb:UpdateItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:us-west-2:*:table/elephant-meetings\"\n        },\n        {\n            \"Sid\": \"AllowEC2Termination\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"ec2:TerminateInstances\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:RequestedRegion\": \"us-west-2\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowCloudWatchLogs\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"arn:aws:logs:us-west-2:*:*\"\n        }\n    ]\n}\n</code></pre> <p>This allows read/write in the DynamoDB, EC2 termination and logging in CloudWatch.</p>"},{"location":"Phase%204/spin-down/#termination-lambda","title":"Termination Lambda","text":"<p>This is the Lambda that will kill beyond the 15 minute mark once data is written for both. It is also the Lambda function that will receive the stream from DynamoDB. We'll do that last.</p>"},{"location":"Phase%204/spin-down/#code-breakdown_1","title":"Code Breakdown","text":"Stream Processing<pre><code>def lambda_handler(event, context):\n    print(f\"Stream Processor: Received {len(event['Records'])} records.\")\n\n    table = dynamodb.Table(TABLE_NAME)\n\n    for record in event['Records']:\n        if record['eventName'] != 'MODIFY':\n            continue\n\n        new_image = record['dynamodb']['NewImage']\n</code></pre> <p>Note - DynamoDB Stream Events</p> <p>The event received is from DynamoDB. It iterates through the batch and discards anything that isn't a <code>MODIFY</code> event (ignoring new inserts or deletions).</p> <p>It captures <code>NewImage</code>, which is the row's data after the update occurred, in DynamoDB JSON format.</p> Checking for Complete Data<pre><code>has_teacher = 'teacher_session_data' in new_image\nhas_student = 'student_session_data' in new_image\n\nif not (has_teacher and has_student):\n    continue\n\n# 2. Extract Meeting ID\nmeeting_id = new_image['id']['S']\n\n# 3. Safety Gate: Duration Check\n# We calculate duration from the scheduled 'meet_time', not 'created_at'\ntry:\n    # Try 'meet_time' first (Correct logic)\n    if 'meet_time' in new_image:\n        time_str = new_image['meet_time']['S']\n    # Fallback to 'created_at' only if meet_time is missing (Legacy support)\n    elif 'created_at' in new_image:\n        time_str = new_image['created_at']['S']\n        print(f\"WARNING: 'meet_time' missing for {meeting_id}. Falling back to 'created_at'.\")\n    else:\n        raise ValueError(\"No timestamp found\")\n\n    # Parse ISO string (handle Z or +00:00)\n    start_time = datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00'))\n\n    # Ensure start_time is timezone-aware (UTC)\n    if start_time.tzinfo is None:\n        start_time = start_time.replace(tzinfo=datetime.timezone.utc)\n\n    now = datetime.datetime.now(datetime.timezone.utc)\n    duration_minutes = (now - start_time).total_seconds() / 60\n</code></pre> <p>Note - Duration Check Logic</p> <p>This entire chunk basically says, \"from the new_image, if more than 15 minutes have gone by since the meeting time, AND both teacher and student have written data, we can kill the process\".</p> Terminating EC2 Instances<pre><code>if 'teacher_ec2_id' in new_image: targets.append(new_image['teacher_ec2_id']['S'])\nif 'student_ec2_id' in new_image: targets.append(new_image['student_ec2_id']['S'])\n\nprint(f\"DEBUG: Targets identified for kill: {targets}\")\n\nterminate_ec2s(targets)\n</code></pre> <p>Here we actually do the termination.</p> Updating Status<pre><code>try:\n    table.update_item(\n        Key={'id': meeting_id},\n        UpdateExpression=\"SET #s = :stat, termination_reason = :reason\",\n        ConditionExpression=\"#s &lt;&gt; :stat\", \n        ExpressionAttributeNames={'#s': 'status'},\n        ExpressionAttributeValues={\n            ':stat': 'TERMINATED', \n            ':reason': 'SUCCESS'\n        }\n    )\n</code></pre> <p>Finally, we write to DynamoDB that the status is <code>TERMINATED</code>, and the reason is <code>SUCCESS</code>.</p>"},{"location":"Phase%204/spin-down/#iam-role_1","title":"IAM Role","text":"<ul> <li>AWSLambdaBasicExecutionRole</li> <li>AWSLambdaDynamoDBExecutionRole</li> <li>LaunchTerminateEC2, custom inline</li> </ul> LaunchTerminateEC2 Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:RunInstances\",\n                \"ec2:CreateTags\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeInstances\",\n                \"ec2:TerminateInstances\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\",\n            \"Resource\": \"arn:aws:iam::034489661489:role/EC2-S3-ReadOnly-Role\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:GetItem\",\n                \"dynamodb:UpdateItem\",\n                \"dynamodb:PutItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/elephant-meetings\"\n        }\n    ]\n}\n</code></pre> <p>Note - Termination Strategy</p> <p>These two functions ensure a semi-smart way to terminate instances. But again, I think that it's over-engineered, if you're not too worried about costs, then you should probably just have the safety net terminate the EC2's after a fixed amount of time.</p> <p>Congratulations! You've completed the Spin Down section. You now have a complete termination system that safely manages EC2 instances based on meeting activity and duration.</p>"},{"location":"Phase%205/","title":"Phase 5: KioskUser and Health Check","text":""},{"location":"Phase%205/#overview","title":"Overview","text":"<p>In this phase, we'll set up a dedicated KioskUser account that will only be able to access the Jitsi server (or whatever app), while OCAP runs in the background and captures desktop inputs. We'll also implement robust health check logic to ensure everything is ready when the user logs in.</p>"},{"location":"Phase%205/#what-youll-learn","title":"What You'll Learn","text":"<p>This phase covers creating a kiosk user and implementing health checks:</p> <ol> <li>Creating KioskUser and Task Scheduler - Create the KioskUser account and configure Task Scheduler</li> <li>Get Meeting Link - Script to fetch meeting link from DynamoDB</li> <li>Health Check - Implement health check logic to warm up the EC2</li> <li>Master Launch, Invisible Shell, and Fallback - Create launch scripts and invisible shell wrapper</li> <li>Autologon - Configure automatic login for KioskUser</li> </ol>"},{"location":"Phase%205/#expected-outcome","title":"Expected Outcome","text":"<p>By the end of this phase, you should have:</p> <ul> <li>A KioskUser account configured with proper permissions</li> <li>Task Scheduler configured for KioskUser</li> <li>Scripts to fetch meeting links from DynamoDB</li> <li>Health check system to ensure EC2 is ready</li> <li>Master launch script with invisible shell wrapper</li> <li>Fallback launch script for workstation unlock</li> <li>Autologon configured for automatic KioskUser login</li> </ul> <p>Let's get started with Creating KioskUser and Task Scheduler \u2192</p>"},{"location":"Phase%205/autologon/","title":"Autologon","text":"<p>Now, we have one thing left to do. We're going to configure AutoLogon for KioskUser in Registry Editor.</p> <p>What AutoLogon does:</p> <ul> <li>The moment an EC2 boots up, before the user even enters, it will log into KioskUser</li> <li>It runs our VBS-wrapped <code>master_launch</code> script, which includes the whole health check</li> <li>When the user actually logs in, the kiosk will be waiting for them</li> <li>If not, the fallback script will reboot it</li> <li>OCAP will trigger no matter what</li> </ul> <p>Warning - Why AutoLogon is Critical</p> <p>I can't stress how necessary this is: on first boot up of an EC2, you can legitimately be waiting 2-3 minutes on a black screen until you see any sign of life, and that does not go away by just letting the EC2 stay idle. It MUST load user profiles into PowerShell, load the conda environment, etc. AutoLogon ensures this \"warming up\" happens automatically.</p> <p>Here's how to configure AutoLogon for KioskUser:</p> <ol> <li>Press Win + R, type <code>regedit</code>, and press Enter.</li> <li>Navigate to the following path:     <pre><code>HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\n</code></pre></li> <li>Modify (or create) the following String Values (REG_SZ):</li> </ol> Value Name Value Data Note AutoAdminLogon <code>1</code> Enables the feature. DefaultUserName <code>KioskUser</code> The target account. DefaultPassword (Your Actual Password) Must be the real password. <p>Note - HKEY_LOCAL_MACHINE vs HKEY_USERS</p> <p>Note that last time, we were configuring for a specific user, hence <code>HKEY_USERS</code>. This is <code>LOCAL_MACHINE</code>, so it's machine wide.</p> <p>Note - Switching Back to Administrator</p> <p>A side inconvenience is that now, when you boot up the EC2, you'll be logged into KioskUser by default. Just hit <code>ctrl + alt + del</code> and Sign Out, switching back into Administrator.</p>"},{"location":"Phase%205/autologon/#full-flow","title":"Full Flow","text":"<p>Here's the complete flow from EC2 boot to user session:</p> <ol> <li>EC2 boots up</li> <li>AutoLogon logs in as KioskUser</li> <li>VBS-wrapped <code>master_launch</code> runs, doing the whole health check and booting up the actual Edge kiosk</li> <li>When the health check finishes, writes \"Healthy\" to the DynamoDB and locks the workstation</li> <li>User logs in, triggering the VBS-wrapped fallback script</li> <li>OCAP starts</li> </ol> <p>Phase 5 Complete</p> <p>And that's it. You've configured KioskUser as a single app kiosk, triggered the master_launch and health check.</p>"},{"location":"Phase%205/get-meeting-link/","title":"Get Meeting Link","text":"<p>We need <code>get_meeting_link.py</code> in our <code>scripts</code> folder. This script will fetch a meeting link from DynamoDB based on which row contains the EC2's instance ID.</p> <p>Here are a few important pieces:</p> Initial Configuration<pre><code>METADATA_URL = \"http://169.254.169.254/latest/meta-data/instance-id\"\nTABLE_NAME = \"elephant-meetings\"\nAWS_REGION = \"us-west-2\"\n\n# --- Set the output file path ---\nOUTPUT_DIR = r\"C:\\scripts\\meeting\"\nOUTPUT_METADATA_PATH = os.path.join(OUTPUT_DIR, \"metadata.env\")\n</code></pre> <ul> <li>Static local variables defining where the metadata is, the name of the DynamoDB table, and the region</li> <li><code>169.254.169.254</code> is the Instance Metadata Service (IMDS)<ul> <li>EC2 instances can call it to find out information about themselves from AWS (cool as heck)</li> </ul> </li> </ul> DynamoDB Scan<pre><code>try:\n    dynamodb = boto3.resource('dynamodb', region_name=AWS_REGION)\n    table = dynamodb.Table(TABLE_NAME)\n\n    # Scan for the EC2 ID in either teacher or student field\n    response = table.scan(\n        FilterExpression=(\n            boto3.dynamodb.conditions.Attr('teacher_ec2_id').eq(instance_id) | \n            boto3.dynamodb.conditions.Attr('student_ec2_id').eq(instance_id)\n        )\n    )\n</code></pre> <ul> <li>This is where we scan for the instance id within DynamoDB</li> </ul> Role Detection and Metadata Extraction<pre><code>role = None\nif item.get('teacher_ec2_id') == instance_id:\n    role = 'teacher'\nelif item.get('student_ec2_id') == instance_id:\n    role = 'student'\n\n# 2. Extract Data (Use safe defaults if keys are missing)\nmetadata = {\n    \"MEETING_URL\": item.get('jitsi_url', 'URL_NOT_FOUND'),\n    \"USER_ROLE\": role,\n    # *** FIX: Use 'id' (Primary Key) instead of 'session_id' ***\n    \"SESSION_ID\": item.get('id', 'ID_NOT_FOUND'),\n    \"TEACHER_NAME\": item.get('teacher_name', 'UnknownTeacher'),\n    \"STUDENT_NAME\": item.get('student_name', 'UnknownStudent'),\n}\n</code></pre> <ul> <li>In the case that there's a match (there should be), we initialize the \"role\" variable</li> <li>Assign the role to either \"teacher\" or \"student\"</li> <li>Then write the <code>metadata.env</code> file</li> <li>By knowing the <code>session_id</code> and the role, we know exactly where to store the link to the uploaded data</li> </ul> <p>Awesome - Jitsi Meeting Provisioning</p> <p>The meeting URL will be used to dynamically determine our Jitsi meeting room. Fun fact about Jitsi: meetings are provisioned on the fly, meaning that we don't need to send any HTTP request saying \"hey Jitsi, spin me up a meeting\". You can go to <code>meet.jitsi.com/vancouveriscool</code> and it will just be there. Kinda sick.</p> Write Metadata File<pre><code>try:\n    # Ensure the directory exists\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Write the new .env file\n    with open(OUTPUT_METADATA_PATH, 'w') as f:\n        for key, value in metadata.items():\n            # Write in strict KEY=VALUE format (no spaces)\n            f.write(f\"{key}={value}\\n\")\n</code></pre> <ul> <li><code>os</code> is the library that allows the function to interact with the environment it is running in. We're creating the <code>metadata.env</code> file in the location we defined above and writing all the items.</li> </ul> <p>Testing - Script Requirements</p> <p>Test this script. The only requirement is that you need a row in DynamoDB with a <code>session_id</code> and the EC2's ID in either the <code>teacher_ec2_id</code> or <code>student_ec2_id</code> column. You can manually put this in easily, which is the upside to DynamoDB.</p>"},{"location":"Phase%205/health-check/","title":"Health Check","text":"<p>Awesome - Health Check System</p> <p>Seriously, the health check stuff is one of my favourite parts of the projects. It addresses some key limitations of using an automatically provisioned EC2.</p> <p>This is basically how an EC2 feels when it is created from an AMI:</p> <p></p> <p>The health checks are going to warm everything up, ensuring that the EC2 is ready when users connect.</p> <p>Note - What is Health Check?</p> <p>Health check is mainly a PowerShell script, with a small counterpart in Python called <code>update_health.py</code>. The Python script simply writes \"Healthy\" to DynamoDB in the row of the session ID, which is the green light for entering into the EC2.</p> <p>The health check has a somewhat complicated role in relation to the master launch script we're going to go through next, but for now, we're going to pretend that it is in a bubble.</p> <p>As mentioned above, EC2's are slow on startup. Especially PowerShell, which loads a bunch of user profiles and default paths. I tried to eliminate PowerShell entirely from this project, which was faster (using <code>cmd.exe</code> and batch scripts are ways around PowerShell), but I ran into a ton of dynamic link library errors.</p> <p></p> <p>Warning - PowerShell Required</p> <p>So, we want PowerShell, but we need to run a PowerShell script\u2014actually run all the scripts we're going to use as a dry run before the user ever gets to us. This \"warming up\" process is critical for performance.</p> <p>I will go through the health check PowerShell script now.</p> Start OCAP Process<pre><code>conda activate ocap-env\n$OcapProc = Start-Process powershell.exe -ArgumentList \"-ExecutionPolicy Bypass\", \"-File C:\\scripts\\ocap.ps1\" -WindowStyle Hidden -PassThru\n</code></pre> <ul> <li>This is basically a PowerShell script running a PowerShell script. Activating conda is probably not necessary because we do the same thing in ocap.ps1, but oh well. Get rid of it if you want.</li> </ul> Smart Waiter<pre><code>$Timeout = 60\n$Timer = [System.Diagnostics.Stopwatch]::StartNew()\n$FileDetected = $false\n\nwhile ($Timer.Elapsed.TotalSeconds -lt $Timeout) {\n    # Check if .mcap or .mkv exists in the temp folder\n    if (Test-Path \"C:\\scripts\\temp_recordings\\*.mcap\" -PathType Leaf) {\n        $FileDetected = $true\n        break\n    }\n    if (Test-Path \"C:\\scripts\\temp_recordings\\*.mkv\" -PathType Leaf) {\n        $FileDetected = $true\n        break\n    }\n    Start-Sleep -Seconds 1\n}\n</code></pre> <ul> <li>This is what I would call a Smart Waiter. The previous step runs OCAP as a dry run, but how do we know when to kill it? Do we just sleep for 30 seconds? 60 seconds? Hell no.</li> </ul> <p>Note - How the Smart Waiter Works</p> <p>We know that once OCAP starts, it sticks the files it is streaming to in our directory <code>temp_recordings</code>. So all we have to do is poll that folder until the files show up, and we know that OCAP is running. If you took CPSC 213 at UBC, you might be thinking, \"Couldn't we have OCAP send an interrupt to the health check script, or use threads to block the current process until OCAP is done starting up\"? Sure. I welcome contributions.</p> Cleanup and Update Health<pre><code>python \"C:\\scripts\\stop_ocap.py\"\nif ($OcapProc) {\n    $OcapProc | Wait-Process -Timeout 15 -ErrorAction SilentlyContinue\n}\n$RecDir = \"C:\\scripts\\temp_recordings\"\nif (Test-Path $RecDir) {\n    Get-ChildItem -Path $RecDir -Include *.* -Recurse | Remove-Item -Force -ErrorAction SilentlyContinue\n}\n\npython \"C:\\scripts\\update_health.py\"\n</code></pre> <ul> <li>Calls our stop script</li> <li>If the OCAP process defined above (first code chunk) is still running, wait another 15 seconds for it to terminate</li> <li>Remove the items in <code>temp_recordings</code></li> <li>Run the update health script that writes \"healthy\" to DynamoDB</li> </ul> <p>Testing - Health Check</p> <p>You should be able to test this whole thing. Make the status in your test row in DynamoDB something like \"unhealthy\", then run health check. It should be able to write \"healthy\", and you should also see OCAP startup and get terminated.</p>"},{"location":"Phase%205/kioskuser-task-scheduler/","title":"Creating KioskUser and Task Scheduler Modification","text":"<p>Alright, the time has come for us to set up KioskUser. This is going to be our second user that will only be able to access the Jitsi server (or whatever app), while OCAP runs in the background and captures desktop inputs.</p> <p>To do this, we're going to:</p> <ol> <li>Create the KioskUser account</li> <li>Configure Task Scheduler for the user</li> <li>Set up folder permissions</li> <li>Configure health check logic (which is necessary to ensure everything is ready when the user logs in)</li> <li>Set it up as a single app kiosk in Registry Editor</li> </ol>"},{"location":"Phase%205/kioskuser-task-scheduler/#creating-the-kioskuser","title":"Creating the KioskUser","text":"<p>This is quite easy. </p> <p>Go to <code>Run</code> and look up <code>lusrmgr.msc</code>. This stands for Local Users and Groups. It should look like this:</p> <p></p> <p>Go ahead and create a <code>new user</code>.</p> <p></p> <p>Then put in a User name and Password. I went with <code>KioskUser</code>.</p> <p>Testing - Login Verification</p> <p>Press <code>ctrl + alt + del</code> and sign out of <code>Administrator</code>. Make sure that you can log into KioskUser. Once you can do that, go back into Administrator.</p> <p>Within <code>C:\\Users</code>, you should be able to see KioskUser now. Right off the bat, you're gonna do something that is a bit wack, but works for this setup.</p> <p>Setting up folder permissions:</p> <ol> <li>Right-click on <code>projects</code> and <code>scripts</code> folders</li> <li>Click <code>Properties</code> and go to <code>Security</code></li> <li>Click <code>Edit</code> to modify permissions</li> </ol> <p></p> <p>Give <code>KioskUser</code> full controls. Then do the same thing for the entire <code>Administrator</code> folder in <code>C:\\Users</code>.</p> <p>Warning - Permission Workaround</p> <p>This whole setup is not super ideal, and you can try avoiding it. However, if you installed Miniconda and your <code>ocap-env</code> in Administrator (like I did), you'll need to give KioskUser access. This was an easier workaround, and KioskUser will not have access to these folders anyway since they're locked down by the kiosk mode.</p>"},{"location":"Phase%205/kioskuser-task-scheduler/#editing-task-scheduler","title":"Editing Task Scheduler","text":"<p>Remember how we configured the tasks to run as Administrator? Just change everything to KioskUser. We will be testing there from now on.</p>"},{"location":"Phase%205/master-launch/","title":"Master Launch, Invisible Shell, and Fallback","text":"<p>We're going to write a few more scripts that are going to perform robust health checks and allow for some pretty cool behaviour in the EC2.</p>"},{"location":"Phase%205/master-launch/#master-launch","title":"Master Launch","text":"<p>Next is <code>master_launch.bat</code>. This script is basically our sequence of behaviour that needs to happen on login; it kind of replaces the logic of just running OCAP when the user joins.</p> <p>Note - Batch Script Syntax</p> <p>This is a batch script, which kind of sucks in terms of syntax, but bear with me.</p> <p>Here are a few critical pieces:</p> Network Check<pre><code>echo [STATUS] Waiting for network...\n:CHECK_NET\nping -n 1 8.8.8.8 &gt;nul 2&gt;&amp;1\nif errorlevel 1 (\n    timeout /t 2 /nobreak &gt;nul\n    goto :CHECK_NET\n)\necho [STATUS] Network Online.\n</code></pre> <ul> <li>This pings Google until it gets a response. It's kind of like a polling to ensure that a new EC2 has an internet connection.</li> </ul> Get Meeting Link<pre><code>\"%TARGET_ENV%\\python.exe\" \"%SCRIPT_ROOT%\\get_meeting_link.py\"\n</code></pre> <ul> <li>This runs the get_meeting_link function, storing metadata in the <code>meetings</code> folder</li> </ul> Launch Kiosk<pre><code>start /wait \"\" \"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\" --kiosk \"%MEETING_URL%\" --edge-kiosk-type=fullscreen --no-first-run\n</code></pre> <ul> <li>This starts the kiosk. Note that in batch, <code>--</code> means that these are arguments. <code>--edge-kiosk-type=fullscreen</code> is self-evident, but <code>--no-first-run</code> makes sure that Edge doesn't go through its usual setup prompts that it would with a new user (what language, make Edge your default browser, can we gather your data, etc.)</li> </ul> Health Check<pre><code>powershell.exe -ExecutionPolicy Bypass -File \"%SCRIPT_ROOT%\\health_check.ps1\"\n</code></pre> <ul> <li>This runs the health check PowerShell script.</li> </ul> Lock Workstation<pre><code>rundll32.exe user32.dll,LockWorkStation\n</code></pre> <ul> <li>Locks the workstation.</li> </ul>"},{"location":"Phase%205/master-launch/#invisible-shell","title":"Invisible Shell","text":"<p>Finally, we have invisible shell. This is going to allow scripts to launch without having the PowerShell or cmd windows obvious to the user. We're gonna write this in VBS, which is an executable that the Registry Editor can execute. Also, hell yeah for Visual Basic!</p> <p></p> <p>This is the VBS script that basically just wraps <code>master_launch.bat</code>.</p> invisible_shell.vbs<pre><code>Set WshShell = CreateObject(\"WScript.Shell\")\nWshShell.Run \"cmd.exe /c C:\\scripts\\master_launch.bat\", 0, True\n</code></pre> <ul> <li><code>0</code> hides the window</li> <li><code>True</code> waits for the script to finish, because if you set this as your shell and it finishes, it will just log you off.</li> <li>As always, this script goes in <code>C:\\scripts</code>.</li> </ul>"},{"location":"Phase%205/master-launch/#registry-editor","title":"Registry Editor","text":"<p>So now we have to do some pretty low-level stuff in order to set up the single app. The full Windows OS has a dedicated single-app kiosk mode, but it's not available on Windows Server 2022. We're going to use an application called Registry Editor.</p> <p>Note - Editing Another User's Registry</p> <p>Normally you have to be logged in as the user to edit their registries, but we're going to cheat. You can load in another user's registry by locating <code>NTUSER.DAT</code>, which will be at the <code>C:\\Users\\KioskUser\\</code> level.</p> <p></p> <p>Here's what you need to do:</p> <ul> <li>Open Regedit:<ul> <li>Press <code>Win + R</code>, type <code>regedit</code>, and hit Enter.</li> </ul> </li> <li>Select the Landing Zone:<ul> <li>Single-click on <code>HKEY_USERS</code> to highlight it.</li> </ul> </li> <li>Load the Hive:<ul> <li>Go to File &gt; Load Hive...</li> <li>Navigate to <code>C:\\Users\\KioskUser\\NTUSER.DAT</code> (Type filename manually if hidden).</li> </ul> </li> <li>Name It:<ul> <li>Enter the Key Name: <code>Kiosk_Edit</code>.</li> </ul> </li> <li>Expand the Folder:<ul> <li>Double-click <code>HKEY_USERS</code> &gt; <code>Kiosk_Edit</code>.</li> </ul> </li> <li> <p>Navigate to WinLogon (The Step You Added):</p> <ul> <li> <p>Drill down into this specific path: <code>Software</code> &gt; <code>Microsoft</code> &gt; <code>Windows NT</code> &gt; <code>CurrentVersion</code> &gt; <code>Winlogon</code></p> </li> <li> <p>Full Path: <code>Computer\\HKEY_USERS\\Kiosk_Edit\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon</code></p> </li> </ul> </li> </ul> <p>Once you're in Winlogon, you'll see a bunch of key/value pairs.</p> <p>Warning - Registry Editing Precision</p> <p>The way this works is that you must be very strict in the names of your keys, and the data you put in. There's not really any compiler here; you're writing very close to assembly code.</p> <p>Right click and create a new String. Call it Shell. Then modify it, and put in this script:</p> Registry Shell Value<pre><code>wscript.exe \"C:\\scripts\\invisible_shell.vbs\"\n</code></pre> <p>This will be our shell script that will totally replace the desktop for KioskUser. When you're done, click the registry and then unload it in the task bar.</p> <p></p> <p>Now the next time you log in as KioskUser, it should only be able to run this script, which in this case, points to an Edge browser; it could be anything, though.</p>"},{"location":"Phase%205/master-launch/#fallback-launch","title":"Fallback Launch","text":"<p>Last thing in this section: we're going to have a cousin of the <code>master_launch</code> script called <code>fallback_launch</code>. </p> <p>What this script does:</p> <ul> <li>Replaces our scheduled task that triggers on workstation unlock</li> <li>Previously, we were just running <code>powershell.exe ocap.ps1</code></li> <li>Now we're going to also reboot the kiosk in case a user closed it previously, or just if anything weird happened</li> </ul> fallback_launch.bat<pre><code>tasklist /FI \"IMAGENAME eq msedge.exe\" 2&gt;NUL | find /I /N \"msedge.exe\"&gt;NUL\nif \"%ERRORLEVEL%\"==\"0\" (\n    echo [STATUS] Kiosk active. Skipping relaunch.\n    goto :LAUNCH_OCAP\n)\n\necho [STATUS] Launching Kiosk...\nstart \"\" \"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\" --kiosk \"%MEETING_URL%\" --edge-kiosk-type=fullscreen --no-first-run\n\n:LAUNCH_OCAP\n:: --- START RECORDING ---\nstart \"OCAP_RECORDER\" powershell.exe -NoExit -WindowStyle Hidden -ExecutionPolicy Bypass -Command \"&amp; '%SCRIPT_ROOT%\\ocap.ps1'\"\n</code></pre> <ul> <li>What this script does is check if an Edge process is running. If yes, it just skips to start OCAP. If not, it will boot one.</li> <li>Then it goes ahead with running OCAP as usual.</li> </ul> <p>We're gonna wrap this in a VBS script just like we did with <code>master_launch</code>. Then we're gonna go into Task Scheduler, and go to our Start OCAP task. Simply point to <code>wscript.exe</code> as the executable, and the path to the script as the arguments.</p> <p></p>"},{"location":"Phase%206/","title":"Phase 6: Optimizations and Config","text":""},{"location":"Phase%206/#overview","title":"Overview","text":"<p>In this last section, we're gonna cover a few ways that you can make the experience better.</p>"},{"location":"Phase%206/#what-youll-learn","title":"What You'll Learn","text":"<p>This phase covers optimizations and configuration improvements:</p> <ol> <li>DCV Performance Settings - Better frame rate and more flexible resolution on large monitors</li> <li>DCV Fullscreen Display - Force fullscreen display with CSS upscaling and smart resolution capping</li> <li>Auto-Enable Webcam and Microphone - Automatic media activation with dual-strategy approach and Jitsi refresh</li> <li>Automated Email Summary - AI-powered session analysis and automated email reports</li> <li>AppLocker Configuration - Restrict applications for KioskUser using Windows AppLocker</li> </ol> <p>Let's get started with DCV Performance Settings \u2192</p>"},{"location":"Phase%206/applocker/","title":"AppLocker Configuration","text":"<p>AppLocker is a Windows method that allows you to block certain applications for certain users, such as Task Manager for KioskUser.</p> <p>Note - Application Identity Service</p> <p>AppLocker requires the \"Application Identity\" service to be running.</p>"},{"location":"Phase%206/applocker/#part-1-block-task-manager-via-applocker","title":"Part 1: Block Task Manager via AppLocker","text":""},{"location":"Phase%206/applocker/#step-1-open-policy-editor","title":"Step 1: Open Policy Editor","text":"<ol> <li>Run <code>secpol.msc</code>.</li> </ol>"},{"location":"Phase%206/applocker/#step-2-create-default-rules-vital","title":"Step 2: Create Default Rules (Vital)","text":"<ol> <li>Navigate to Application Control Policies &gt; AppLocker &gt; Executable Rules.</li> <li>Right-click and select Create Default Rules (prevents you from locking yourself out).</li> <li>Also do it for Script Rules. Some of our code counts as an executable, some as scripts.</li> </ol> <p>This is what mine looks like.</p>"},{"location":"Phase%206/applocker/#executable-rules","title":"Executable Rules","text":"Action User Name/Path Condition \u2705 Allow Everyone <code>C:\\Users\\Administrator\\Miniconda3\\*</code> Path \u2705 Allow Everyone <code>C:\\projects\\*</code> Path \u2705 Allow Everyone (Default Rule) All files located in the Program Files folder Path \u2705 Allow Everyone <code>C:\\Users\\KioskUser\\*</code> Path \u2705 Allow Everyone (Default Rule) All files located in the Windows folder Path \ud83d\udeab Deny EC2AMAZ-GPDFMFQ\\KioskUser <code>C:\\Windows\\System32\\Taskmgr.exe</code> Path \u2705 Allow Everyone <code>C:\\scripts\\*</code> Path \u2705 Allow BUILTIN\\Administrators (Default Rule) All files Path"},{"location":"Phase%206/applocker/#script-rules","title":"Script Rules","text":"Action User Name/Path Condition \u2705 Allow Everyone <code>C:\\scripts\\*</code> Path \u2705 Allow Everyone (Default Rule) All scripts located in the Program Files folder Path \u2705 Allow Everyone <code>C:\\Users\\Administrator\\Miniconda3\\Scripts\\*</code> Path \u2705 Allow Everyone <code>%OSDRIVE%\\USERS\\KIOSKUSER\\APPDATA\\LOCAL\\TEMP\\*</code> Path \u2705 Allow Everyone <code>C:\\Users\\Administrator\\Miniconda3\\*</code> Path \u2705 Allow EC2AMAZ-GPDFMFQ\\KioskUser <code>C:\\Users\\KioskUser*</code> Path \u2705 Allow Everyone (Default Rule) All scripts located in the Windows folder Path \u2705 Allow Everyone <code>C:\\Users\\Administrator\\Miniconda3\\condabin\\*</code> Path \u2705 Allow BUILTIN\\Administrators (Default Rule) All scripts Path"},{"location":"Phase%206/applocker/#step-3-create-deny-rule","title":"Step 3: Create Deny Rule","text":"<ol> <li>Right-click &gt; Create New Rule.</li> <li>Action: Deny.</li> <li>User: Select the specific Student/User.</li> <li>Conditions: Path &gt; Browse Files &gt; <code>C:\\Windows\\System32\\Taskmgr.exe</code>.</li> <li>Click Create.</li> </ol>"},{"location":"Phase%206/applocker/#step-4-enforce-policy","title":"Step 4: Enforce Policy","text":"<ol> <li>Right-click AppLocker (the root node) &gt; Properties.</li> <li>Check Configured under Executable rules and set to Enforce rules.</li> </ol>"},{"location":"Phase%206/applocker/#step-5-start-service-cmd","title":"Step 5: Start Service (CMD)","text":"<p>Run this as Admin to ensure the AppLocker engine is on:</p> Start AppLocker Service<pre><code>sc start AppIDSvc\ngpupdate /force\n</code></pre> <p>Warning - Start Menu Impact</p> <p>This will probably make your start menu unusable. You'll have to boot stuff from cmd.exe while it's on.</p>"},{"location":"Phase%206/applocker/#looking-at-events-via-event-viewer","title":"Looking at Events via Event Viewer","text":"<p>Awesome - Event Viewer Monitoring</p> <p>This is SUPER helpful to monitor AppLocker.</p> <p></p> <p>Here is how to check the AppLocker logs using the Event Viewer GUI:</p> <ol> <li>Press Win + R, type <code>eventvwr.msc</code>, and hit Enter.</li> <li>Navigate to this specific folder tree:    Applications and Services Logs &gt; Microsoft &gt; Windows &gt; AppLocker</li> <li>Click on EXE and DLL.</li> <li>Look for Event ID 8004.</li> <li>8004: Application was blocked.</li> <li>8002: Application was allowed.</li> </ol> <p>The \"General\" tab in the bottom pane will tell you exactly which file was blocked and which user tried to run it.</p> <p>Phase Complete</p> <p>You've completed Phase 6. Your system is now optimized with better DCV performance settings and secure application restrictions.</p>"},{"location":"Phase%206/auto-enable-media/","title":"Auto-Enable Webcam and Microphone","text":"<p>This section covers the implementation of automatic webcam and microphone activation in the DCV frontend, including the dual-strategy approach for reliable device activation and the Jitsi refresh mechanism.</p>"},{"location":"Phase%206/auto-enable-media/#overview","title":"Overview","text":"<p>The auto-enable feature ensures that webcam and microphone are automatically activated when a DCV session connects, eliminating the need for manual user intervention. This is critical for seamless integration with Jitsi Meet, which requires active media streams to function properly.</p>"},{"location":"Phase%206/auto-enable-media/#architecture","title":"Architecture","text":"<p>The implementation uses a dual-strategy approach to handle different scenarios:</p> <ol> <li>Explicit Feature Announcements - Reacts to server announcements via <code>featuresUpdate</code> callback</li> <li>Blind Auto-Start - Attempts to enable devices after a delay, regardless of announcements</li> </ol> <p>This redundancy ensures devices activate even when the server doesn't explicitly announce capabilities or when callbacks are missed.</p>"},{"location":"Phase%206/auto-enable-media/#implementation-details","title":"Implementation Details","text":""},{"location":"Phase%206/auto-enable-media/#1-features-update-callback","title":"1. Features Update Callback","text":"<p>The <code>featuresUpdate</code> callback listens for explicit server announcements about available features:</p> <pre><code>featuresUpdate: function (features) {\n    console.log(\"Server Feature Update:\", features);\n    // Redundancy: If server explicitly says \"ready\", try to start immediately\n    if (features.webcam) {\n        connection.setWebcam(true).catch(e =&gt; console.warn(\"Webcam start retry:\", e.message));\n    }\n    if (features['audio-in']) {\n        connection.setMicrophone(true).catch(e =&gt; console.warn(\"Mic start retry:\", e.message));\n    }\n}\n</code></pre> <p>Why this is needed: Some DCV servers explicitly announce when features are ready. This callback provides immediate activation when such announcements occur.</p>"},{"location":"Phase%206/auto-enable-media/#2-blind-auto-start-logic","title":"2. Blind Auto-Start Logic","text":"<p>After the connection is established, a 2-second delay triggers a \"blind\" attempt to enable devices:</p> <pre><code>setTimeout(() =&gt; {\n    console.log(\"Attempting blind auto-start of devices...\");\n\n    connection.setMicrophone(true).catch(e =&gt; console.warn(e));\n\n    connection.setWebcam(true).then(() =&gt; {\n        console.log(\"Webcam Started. Waiting 1s for driver, then kicking Jitsi...\");\n\n        // Wait 1 second for the virtual driver to mount, then refresh\n        setTimeout(() =&gt; {\n            // Send F5 refresh to wake up Jitsi\n            // ... (see F5 Refresh section below)\n        }, 1000);\n    }).catch(e =&gt; console.warn(\"Webcam start error:\", e.message));\n}, 2000); // 2 second delay\n</code></pre> <p>Why this is needed: Some devices (especially audio) may be created silently without triggering feature announcements. The blind auto-start ensures these devices are activated regardless.</p>"},{"location":"Phase%206/auto-enable-media/#3-f5-refresh-mechanism-the-kick","title":"3. F5 Refresh Mechanism (\"The Kick\")","text":"<p>After the webcam is successfully enabled, the system automatically sends an F5 keypress to refresh the remote browser window. This is critical because Jitsi may have loaded before the webcam driver was ready.</p> <pre><code>connection.setWebcam(true).then(() =&gt; {\n    setTimeout(() =&gt; {\n        if (typeof connection.sendKeyboardEvent === 'function') {\n            // Create F5 keydown event\n            const keyDownEvent = new KeyboardEvent('keydown', {\n                key: 'F5',\n                code: 'F5',\n                keyCode: 116,\n                which: 116,\n                bubbles: true,\n                cancelable: true\n            });\n\n            // Create F5 keyup event\n            const keyUpEvent = new KeyboardEvent('keyup', {\n                key: 'F5',\n                code: 'F5',\n                keyCode: 116,\n                which: 116,\n                bubbles: true,\n                cancelable: true\n            });\n\n            connection.sendKeyboardEvent(keyDownEvent);\n            setTimeout(() =&gt; {\n                connection.sendKeyboardEvent(keyUpEvent);\n                console.log(\"\ud83d\ude80 F5 Refresh Sent!\");\n            }, 50);\n        }\n    }, 1000);\n});\n</code></pre> <p>Why this is needed:  - Jitsi may load before the virtual webcam driver is fully mounted - The refresh ensures Jitsi detects the webcam after it's ready - Prevents \"Unable to access camera\" errors in Jitsi</p> <p>Timing: The refresh happens 1 second after webcam activation to allow the virtual driver to mount.</p>"},{"location":"Phase%206/auto-enable-media/#4-pre-flight-media-permissions","title":"4. Pre-Flight Media Permissions","text":"<p>Before connecting to DCV, users must grant permissions for webcam, microphone, and clipboard. The implementation includes:</p>"},{"location":"Phase%206/auto-enable-media/#individual-enable-buttons","title":"Individual Enable Buttons","text":"<ul> <li>Enable Webcam - Requests camera permission and verifies the stream</li> <li>Enable Microphone - Requests microphone permission and verifies the stream  </li> <li>Enable Clipboard - Requests clipboard write permission</li> </ul>"},{"location":"Phase%206/auto-enable-media/#allow-all-button","title":"\"Allow All\" Button","text":"<p>A convenient one-click option that enables all three permissions sequentially:</p> <pre><code>enableAllBtn.onclick = async (e) =&gt; {\n    // Sequentially enable webcam, mic, and clipboard\n    // Handles errors gracefully (continues if one fails)\n    // Updates button state when all are enabled\n};\n</code></pre> <p>Features: - Enables all permissions in one click - Continues even if one permission fails - Shows \"All Enabled\" state when complete - Disables button when all permissions are granted</p>"},{"location":"Phase%206/auto-enable-media/#5-stream-management","title":"5. Stream Management","text":"<p>Media streams remain active after enabling to provide user feedback:</p> <ul> <li>During Pre-Flight: Streams stay active so users can see/hear their devices working</li> <li>Before Launch: Streams are cleaned up via <code>cleanupStreams()</code> method</li> <li>During DCV Session: DCV takes over the media streams automatically</li> </ul> <pre><code>// Cleanup function stops all active streams before DCV takes over\nfunction cleanupStreams() {\n    if (webcamStream) {\n        webcamStream.getTracks().forEach(track =&gt; track.stop());\n        webcamStream = null;\n    }\n    if (micStream) {\n        micStream.getTracks().forEach(track =&gt; track.stop());\n        micStream = null;\n    }\n}\n</code></pre>"},{"location":"Phase%206/auto-enable-media/#configuration-options","title":"Configuration Options","text":""},{"location":"Phase%206/auto-enable-media/#toggle-f5-refresh","title":"Toggle F5 Refresh","text":"<p>To disable the automatic F5 refresh, you can modify the code:</p> <pre><code>const ENABLE_JITSI_AUTO_REFRESH = false; // Set to false to disable\n\nif (ENABLE_JITSI_AUTO_REFRESH) {\n    // ... F5 refresh code\n}\n</code></pre>"},{"location":"Phase%206/auto-enable-media/#adjust-auto-start-delay","title":"Adjust Auto-Start Delay","text":"<p>The 2-second delay can be modified if needed:</p> <pre><code>setTimeout(() =&gt; {\n    // ... auto-start code\n}, 2000); // Change this value to adjust delay\n</code></pre>"},{"location":"Phase%206/auto-enable-media/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Phase%206/auto-enable-media/#webcam-not-activating","title":"Webcam Not Activating","text":"<ol> <li>Check Server-Side Configuration:</li> <li>Verify Windows Privacy Settings allow camera access for desktop apps</li> <li>Ensure NICE DCV Virtual Webcam driver is installed (check Device Manager)</li> <li> <p>Verify DCV Server permissions file allows webcam</p> </li> <li> <p>Check Browser Console:</p> </li> <li>Look for \"Server Feature Update\" logs</li> <li>Check for \"Webcam auto-start\" messages</li> <li> <p>Verify F5 refresh is being sent</p> </li> <li> <p>Check Server Logs:</p> </li> <li>Look for <code>[channelfactory] Created channel of type 'video'</code> messages</li> <li>Verify no blocking errors in DCV Server logs</li> </ol>"},{"location":"Phase%206/auto-enable-media/#microphone-not-activating","title":"Microphone Not Activating","text":"<ol> <li>Check Server-Side Configuration:</li> <li>Verify Windows Privacy Settings allow microphone access for desktop apps</li> <li> <p>Check that microphone is enabled in DCV Server configuration</p> </li> <li> <p>Check Browser Console:</p> </li> <li>Look for \"Mic auto-start\" messages</li> <li> <p>Verify no error messages in console</p> </li> <li> <p>Check Server Logs:</p> </li> <li>Look for <code>[channelfactory] Created channel of type 'audio'</code> messages</li> </ol>"},{"location":"Phase%206/auto-enable-media/#jitsi-not-detecting-webcam","title":"Jitsi Not Detecting Webcam","text":"<ol> <li>Verify F5 Refresh:</li> <li>Check console for \"\ud83d\ude80 F5 Refresh Sent!\" message</li> <li> <p>Ensure browser window is in focus when refresh is sent</p> </li> <li> <p>Check Timing:</p> </li> <li>The refresh happens 1 second after webcam activation</li> <li> <p>If Jitsi loads very slowly, you may need to increase the delay</p> </li> <li> <p>Manual Refresh:</p> </li> <li>If automatic refresh fails, manually refresh the Jitsi page (F5)</li> <li>The webcam should be detected after refresh</li> </ol> <p>Next: Automated Email summary \u2192</p>"},{"location":"Phase%206/dcv-fullscreen/","title":"Force Fullscreen Display","text":"<p>This section covers how to force the DCV display to fill the entire viewport using client-side CSS upscaling and smart resolution capping.</p>"},{"location":"Phase%206/dcv-fullscreen/#overview","title":"Overview","text":"<p>By default, DCV may display at a resolution that doesn't match your browser window, causing black bars or distorted images. This solution uses CSS to stretch the stream and JavaScript to request resolutions that stay within server limits while maintaining aspect ratio.</p>"},{"location":"Phase%206/dcv-fullscreen/#css-configuration","title":"CSS Configuration","text":"<p>Add the following to <code>frontend/src/css/index.css</code>:</p> <pre><code>#dcv-display {\n    position: fixed;\n    top: 0;\n    left: 0;\n    width: 100vw;\n    height: 100vh;\n    margin: 0;\n    padding: 0;\n    overflow: hidden;\n    background-color: #ffffff;\n}\n\n/* Force video/canvas elements to fill container */\n#dcv-display video,\n#dcv-display canvas,\n#dcv-display &gt; div {\n    width: 100% !important;\n    height: 100% !important;\n    object-fit: contain !important; /* 'contain' maintains aspect ratio, 'fill' stretches */\n    position: absolute !important;\n    top: 0 !important;\n    left: 0 !important;\n}\n</code></pre> <p>Key settings: - <code>object-fit: contain</code> - Maintains aspect ratio (no distortion) - <code>object-fit: fill</code> - Stretches to fill (may distort on mismatched aspect ratios) - White background ensures letterboxing appears as bars</p>"},{"location":"Phase%206/dcv-fullscreen/#javascript-resolution-handling","title":"JavaScript Resolution Handling","text":"<p>Update <code>updateDcvResolution()</code> in <code>frontend/src/js/main.js</code> to cap requests within server limits:</p> <pre><code>function updateDcvResolution() {\n    if (!connection) return;\n\n    const elem = document.getElementById(\"dcv-display\");\n    if (!elem) return;\n\n    const pixelRatio = window.devicePixelRatio || 1;\n    let width = Math.floor(elem.clientWidth * pixelRatio);\n    let height = Math.floor(elem.clientHeight * pixelRatio);\n\n    const originalWidth = width;\n    const originalHeight = height;\n\n    // Smart cap: Keep aspect ratio, stay under server limits\n    const MAX_W = 1920;\n    const MAX_H = 1080;\n\n    if (width &gt; MAX_W) {\n        const scale = MAX_W / width;\n        width = Math.floor(width * scale);\n        height = Math.floor(height * scale);\n    }\n\n    if (height &gt; MAX_H) {\n        const scale = MAX_H / height;\n        width = Math.floor(width * scale);\n        height = Math.floor(height * scale);\n    }\n\n    console.log(`Requesting DCV resolution: ${width}x${height} (Original: ${originalWidth}x${originalHeight})`);\n\n    connection.requestResolution(width, height).catch(e =&gt; {\n        console.error(\"Error requesting resolution: \", e.message);\n    });\n}\n</code></pre> <p>How it works: - Calculates desired resolution including pixel ratio (for Retina/high-DPI displays) - Scales down proportionally if either dimension exceeds server limits (1920x1080) - Maintains aspect ratio to prevent black bars - Logs both final and original requests for debugging</p>"},{"location":"Phase%206/dcv-fullscreen/#why-this-works","title":"Why This Works","text":"<ol> <li>CSS upscaling: Forces browser to stretch DCV stream to viewport size</li> <li>Smart capping: Requests resolutions server accepts while preserving aspect ratio</li> <li>Aspect ratio match: Server sends image matching window shape, CSS displays it correctly</li> </ol>"},{"location":"Phase%206/dcv-fullscreen/#result","title":"Result","text":"<ul> <li>Full-screen display without black bars (when aspect ratios match)</li> <li>No image distortion (with <code>contain</code>)</li> <li>Works across different window sizes and device pixel ratios</li> <li>Lower resolution than native but acceptable quality</li> </ul> <p>Next: Auto-Enable Webcam and Microphone \u2192</p>"},{"location":"Phase%206/dcv-performance/","title":"Better Frame Rate and More Flexible Resolution","text":"<p>This section covers how to configure DCV for better frame rates and support for large monitors.</p> <p></p>"},{"location":"Phase%206/dcv-performance/#registry-configuration","title":"Registry Configuration","text":""},{"location":"Phase%206/dcv-performance/#1-navigate-to-registry-key","title":"1. Navigate to Registry Key","text":"<p>Navigate to: <pre><code>HKEY_USERS\\S-1-5-18\\Software\\GSettings\\com\\nicesoftware\\dcv\\display\n</code></pre></p>"},{"location":"Phase%206/dcv-performance/#2-set-values","title":"2. Set Values","text":"<p>Right-click &gt; New to create these entries inside <code>display</code>:</p> Name Type Value <code>target-fps</code> DWORD (32-bit) <code>0</code> (Uncapped) or <code>60</code> <code>max-head-resolution</code> String Value <code>4096x4096</code>"},{"location":"Phase%206/dcv-performance/#3-apply-changes","title":"3. Apply Changes","text":"<ol> <li>Press Win + R, type <code>services.msc</code>, and hit Enter.</li> <li>Locate DCV Server in the list.</li> <li>Right-click it and select Restart.</li> </ol> <p>Next: DCV Fullscreen Display \u2192</p>"},{"location":"Phase%206/mix-ai-email/","title":"Automated Email Summary","text":"<p>This here is a lil' bonus feature now that you have all this infrastructure set up. In one lambda, you can process the MKV file resulting from the session (I use the teacher one) to mix the audio channels, and then send it to Gemini, which handles video submissions by capture a frame for every second of video and interleaving it with the transcript. Then Gemini can return inline HTML, which is the accepted format by email clients for rendering impressive visuals, such as with newsletters.</p> <p>This Lambda is triggered from DynamoDB stream events (just like the termination lambda). It's triggered automatically when session data is uploaded to S3 and written to DynamoDB.</p> <p>Note - Docker-based Lambda</p> <p>This Lambda uses a Docker container image because it requires FFmpeg for video processing and the Gemini API. The container includes FFmpeg binaries and Python dependencies. When dependencies are needed outside the ones packaged with the Lambda environment, you have to use a Dockerfile and deployment script. This approach allows you to version control locally, which solves the issue of having different snapshots of the lambda locally vs the cloud.</p>"},{"location":"Phase%206/mix-ai-email/#enable-ses","title":"Enable SES","text":"<p>SES (Simple email service) is a way for you to send emails via AWS services. You need to get approved to use it in production (and you might need a business email), but for the sake of testing, you can use sandbox right away. We'll use sandbox.</p> <p></p> <ul> <li>Go select identities, and then hit the Orange 'Create Identity' button.</li> <li>Select the Email Address radio button</li> <li>Enter your email address and confirm</li> </ul> <p>This will allow you to send sandbox emails to your personal email.</p>"},{"location":"Phase%206/mix-ai-email/#create-a-new-lambda","title":"Create a new lambda","text":"<p>You'll need to create a new blank lambda, and select Container image. Use these settings here. You basically just need to pick a name and architecture (I always go x86_64).</p>"},{"location":"Phase%206/mix-ai-email/#structure-of-the-directory","title":"Structure of the directory","text":"<p>Here's what the directory looks like:</p> <pre><code>lambdas/mix-ai-email/\n\u251c\u2500\u2500 Dockerfile             # Container image definition\n\u251c\u2500\u2500 lambda_function.py     # Main Lambda handler code\n\u251c\u2500\u2500 requirements.txt       # Python dependencies\n\u2514\u2500\u2500 deploy.sh              # Deployment script for building and pushing\n</code></pre>"},{"location":"Phase%206/mix-ai-email/#environment-variables","title":"Environment Variables","text":"Key Value GEMINI_API_KEY Your Google Gemini API key <p>Note - Gemini API Key</p> <p>You'll need to obtain a Gemini API key from Google Cloud Console. This is super easy and you get 20 free requests per day. Sick!</p>"},{"location":"Phase%206/mix-ai-email/#dynamodb-stream-trigger","title":"DynamoDB Stream Trigger","text":"<p>This Lambda is triggered by DynamoDB Streams. You need to configure the stream trigger in the Lambda console:</p> <ol> <li>Go to your Lambda function in the AWS Console.</li> <li>Click Add trigger.</li> <li>Select DynamoDB.</li> <li>Choose your <code>elephant-meetings</code> table.</li> <li>Set Batch size to <code>10</code> (or your preference).</li> <li>Enable the trigger.</li> </ol> <p>Note - Stream Events</p> <p>The Lambda processes <code>INSERT</code> and <code>MODIFY</code> events from DynamoDB. It looks for records where <code>teacher_session_data</code> contains an S3 URL and <code>teacher_email</code> is present.</p>"},{"location":"Phase%206/mix-ai-email/#code-breakdown","title":"Code Breakdown","text":"Initial Setup<pre><code>OUTPUT_BUCKET = \"elephant-bucket-ai-summaries\"\nGEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\ngenai.configure(api_key=GEMINI_API_KEY)\n\ns3 = boto3.client('s3')\nses = boto3.client('ses')\n</code></pre> <ul> <li><code>OUTPUT_BUCKET</code> is where the generated HTML reports are stored. This is totally optional, I just thought it might be useful for debugging.</li> <li><code>genai</code> is the Google Generative AI library, configured with your API key.</li> <li><code>ses</code> is AWS Simple Email Service for sending emails.</li> </ul> Lambda Handler - Stream Processing<pre><code>def lambda_handler(event, context):\n    for record in event['Records']:\n        if record['eventName'] in ['INSERT', 'MODIFY']:\n            new_image = record['dynamodb']['NewImage']\n            s3_url = new_image.get('teacher_session_data', {}).get('S', '')\n            email = new_image.get('teacher_email', {}).get('S', '')\n\n            if not email:\n                print(\"No teacher email found, skipping lambda\")\n                continue\n</code></pre> <p>Note - Early Exit Optimization</p> <p>The Lambda skips processing if no teacher email is found. This saves compute costs by avoiding unnecessary video processing when emails aren't needed. You could even make it conditional such that if there isn't student and teacher data, it does nothing.</p> S3 URL Parsing<pre><code>if s3_url.startswith('s3://'):\n    # Handle \"s3://bucket/key\"\n    parts = s3_url.replace(\"s3://\", \"\").split(\"/\", 1)\n    input_bucket = parts[0]\n    input_key = parts[1]\n\nelif s3_url.startswith('https://'):\n    # Handle \"https://bucket.s3.region.amazonaws.com/key\"\n    parsed = urlparse(s3_url)\n    input_bucket = parsed.netloc.split('.')[0]\n    input_key = parsed.path.lstrip('/')\n</code></pre> <ul> <li>The Lambda handles both <code>s3://</code> and <code>https://</code> S3 URL formats.</li> <li>It extracts the bucket name and object key for downloading.</li> </ul> Session Processing Flow<pre><code>def process_session(bucket, key, teacher_email, student_name):\n    # A. Download ZIP from S3\n    s3.download_file(bucket, key, local_zip)\n\n    # B. Unzip &amp; Find MKV\n    with zipfile.ZipFile(local_zip, 'r') as zip_ref:\n        zip_ref.extractall(extract_dir)\n\n    # C. Fuse Audio Tracks\n    fuse_audio_tracks(local_input_mkv, local_fused)\n\n    # D. Upload to Gemini\n    video_file = genai.upload_file(path=local_fused)\n\n    # E. Generate HTML Report\n    model = genai.GenerativeModel(model_name=\"gemini-flash-latest\")\n    response = model.generate_content([video_file, prompt])\n\n    # F. Save Report to S3\n    s3.put_object(Bucket=OUTPUT_BUCKET, Key=output_key, Body=html_content)\n\n    # G. Send Email via SES\n    send_email_via_ses(teacher_email, html_content, student_name)\n</code></pre> <p>Awesome - Complete Automation</p> <p>This Lambda automates the entire workflow: download, process, analyze, store, and email. This borders on \"god lambda\" which might not be ideal for production, BUT it's fast and simple for our purposes</p>"},{"location":"Phase%206/mix-ai-email/#audio-fusion-logic","title":"Audio Fusion Logic","text":"Fuse Audio Tracks<pre><code>def fuse_audio_tracks(input_path, output_path):\n    # 1. FFprobe: Find all audio tracks\n    cmd_probe = ['ffprobe', '-v', 'quiet', '-print_format', 'json', \n                 '-show_streams', str(input_path)]\n\n    # 2. Build FFmpeg Command\n    if len(audio_indices) &gt; 1:\n        # Complex Mix: Multiple audio tracks\n        inputs = \"\".join([f\"[0:a:{i}]\" for i in audio_indices])\n        filter_complex = f\"{inputs}amix=inputs={len(audio_indices)}[a_out]\"\n    else:\n        # Simple Case: Downmix to mono\n        ffmpeg_cmd.extend(['-ac', '1'])\n</code></pre> <p>Note - Audio Processing</p> <p>The Lambda uses FFmpeg to mix multiple audio tracks (teacher and student) into a single track. This ensures Gemini can process the complete audio context. If there's only one track, it downmixes to mono for compatibility.</p>"},{"location":"Phase%206/mix-ai-email/#gemini-ai-analysis","title":"Gemini AI Analysis","text":"AI Prompt<pre><code>prompt = \"\"\"\nYou are an expert educational analyst. \nAnalyze the video and generate a **formatted HTML email body** for the student.\n\nSTYLING:\n- Use inline CSS (e.g., &lt;div style='font-family: sans-serif; max-width: 600px; margin: auto; border: 1px solid #ddd; padding: 20px;'&gt;).\n- Header: '\ud83d\udc18 Elephant Session Report' (Green background #4CAF50, white text).\n- Content: Executive Summary, What We Covered (Bullets), Cheatsheet (HTML Table with Question, Student Response, Correctness), Areas for the Student to Improve.\n- No Markdown. No LaTeX. Raw HTML only.\n\"\"\"\n</code></pre>"},{"location":"Phase%206/mix-ai-email/#email-sending","title":"Email Sending","text":"SES Email Configuration<pre><code>def send_email_via_ses(recipient, html_body, student_name):\n    sender = \"Elephant AI &lt;christardy99@gmail.com&gt;\"\n    subject = f\"Your Tutoring Session Recap with {student_name}\"\n\n    # Add List-Unsubscribe header for email compliance\n    msg.add_header('List-Unsubscribe', unsub_header)\n    msg.add_header('List-Unsubscribe-Post', 'List-Unsubscribe=One-Click')\n\n    # Send via SES\n    ses.send_raw_email(\n        Source=sender,\n        Destinations=[recipient],\n        RawMessage={'Data': msg.as_string()}\n    )\n</code></pre> <p>Note - Email Compliance</p> <p>The Lambda includes <code>List-Unsubscribe</code> headers required by email providers like Gmail. This ensures emails aren't marked as spam and comply with email marketing regulations.</p>"},{"location":"Phase%206/mix-ai-email/#docker-deployment","title":"Docker Deployment","text":"<p>This Lambda uses a Docker container because it requires FFmpeg. The deployment process:</p>"},{"location":"Phase%206/mix-ai-email/#dockerfile-structure","title":"Dockerfile Structure","text":"Dockerfile<pre><code>FROM public.ecr.aws/lambda/python:3.14\n\n# Install system tools (unzip is critical)\nRUN dnf install -y wget tar xz unzip\n\n# Install Static FFmpeg\nRUN wget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz \\\n    &amp;&amp; tar -xf ffmpeg-release-amd64-static.tar.xz \\\n    &amp;&amp; mv ffmpeg-*-amd64-static/ffmpeg /usr/bin/ffmpeg \\\n    &amp;&amp; mv ffmpeg-*-amd64-static/ffprobe /usr/bin/ffprobe \\\n    &amp;&amp; chmod +x /usr/bin/ffmpeg /usr/bin/ffprobe\n\n# Python Dependencies\nCOPY requirements.txt ${LAMBDA_TASK_ROOT}\nRUN pip install -r requirements.txt\n\n# Lambda Code\nCOPY lambda_function.py ${LAMBDA_TASK_ROOT}\n</code></pre> <p>Note - FFmpeg Installation</p> <p>The Dockerfile installs a static FFmpeg binary that doesn't require system libraries. This ensures video processing works reliably in the Lambda environment.</p>"},{"location":"Phase%206/mix-ai-email/#deployment-script","title":"Deployment Script","text":"<p>The <code>deploy.sh</code> script handles: 1. Building the Docker image for Lambda (AMD64 platform) 2. Pushing to Amazon ECR (Elastic Container Registry) 3. Updating the Lambda function code</p> Deploy Command<pre><code>./deploy.sh\n</code></pre> <p>Warning - AWS CLI Requirements</p> <p>You'll need to install the AWS CLI to run the deployment script. You'll probably run into AWS CLI permissions issues, because you'll need IAM permissions not yet configured. I would just say to address them as they come up.</p>"},{"location":"Phase%206/mix-ai-email/#iam-role","title":"IAM Role","text":"<ul> <li>AWSLambdaBasicExecutionRole, pre-built by AWS</li> <li>S3ReadWrite, custom inline policy</li> <li>SESEmailSend, custom inline policy</li> <li>DynamoDBStreamRead, custom inline policy</li> </ul> S3ReadWrite Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::elephant-bucket-ai-summaries/*\",\n                \"arn:aws:s3:::your-session-bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre> SESEmailSend Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ses:SendEmail\",\n                \"ses:SendRawEmail\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> DynamoDBStreamRead Policy<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:DescribeStream\",\n                \"dynamodb:GetRecords\",\n                \"dynamodb:GetShardIterator\",\n                \"dynamodb:ListStreams\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:us-west-2:*:table/elephant-meetings/stream/*\"\n        }\n    ]\n}\n</code></pre> <p>Warning - SES Verification</p> <p>Before sending emails, you must verify your sender email address in SES. Go to SES Console &gt; Verified identities and verify <code>christardy99@gmail.com</code> (or your sender email). If you're in the SES sandbox, you'll also need to verify recipient addresses.</p>"},{"location":"Phase%206/mix-ai-email/#resource-configuration","title":"Resource Configuration","text":""},{"location":"Phase%206/mix-ai-email/#memory-and-timeout","title":"Memory and Timeout","text":"<ul> <li>Memory: <code>3008 MB</code> (recommended for video processing)</li> <li>Timeout: <code>15 minutes</code> (maximum Lambda timeout)</li> </ul> <p>Note - Resource Requirements</p> <p>Video processing and AI analysis are compute-intensive. Higher memory allocation helps with FFmpeg processing and reduces execution time.</p>"},{"location":"Phase%206/mix-ai-email/#ephemeral-storage","title":"Ephemeral Storage","text":"<ul> <li>Ephemeral Storage: <code>10240 MB</code> (10 GB)</li> </ul> <p>Note - Storage Requirements</p> <p>The Lambda downloads ZIP files, extracts videos, and processes them. Larger storage prevents out-of-space errors during processing.</p>"},{"location":"Phase%206/mix-ai-email/#testing","title":"Testing","text":"<p>After deploying the Lambda and configuring the DynamoDB stream trigger, you can verify that everything is working correctly by checking CloudWatch logs and the resulting email output.</p>"},{"location":"Phase%206/mix-ai-email/#cloudwatch-logs","title":"CloudWatch Logs","text":"<p>To monitor the Lambda execution and debug any issues, access CloudWatch logs through the Lambda console:</p> <ol> <li>Go to your Lambda function in the AWS Console.</li> <li>Click on the Monitor tab.</li> <li>Click View CloudWatch logs to see detailed execution logs.</li> </ol> <p></p> <p>The logs will show: - Stream event processing - S3 download progress - FFmpeg audio fusion status - Gemini API upload and processing - Email sending confirmation - Any errors or warnings</p> <p>Awesome - Debugging Tips</p> <p>If the Lambda fails, check the CloudWatch logs for specific error messages. Common issues include missing S3 permissions, invalid Gemini API key, or SES verification problems.</p>"},{"location":"Phase%206/mix-ai-email/#email-output","title":"Email Output","text":"<p>When the Lambda successfully processes a session, it sends a formatted HTML email report to the teacher. The email includes:</p> <ul> <li>Executive Summary - Overview of the session</li> <li>What We Covered - Bullet points of topics discussed</li> <li>Cheatsheet - HTML table with questions, student responses, and correctness</li> <li>Areas for Improvement - Actionable feedback for the student</li> </ul> <p></p> <p>Nice! And that's our basic AI summary.</p> <p>Next: AppLocker Configuration \u2192</p>"}]}